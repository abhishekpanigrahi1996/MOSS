{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BjjTqf2nfCx4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import functools\n",
        "import random\n",
        "print = functools.partial(print, flush=True)\n",
        "\n",
        "def _random_block_scores(num_blocks: int):\n",
        "    \"\"\"Return a list of *num_blocks* floats sampled uniformly in [0,1).\"\"\"\n",
        "    return [random.random() for _ in range(num_blocks)]\n",
        "\n",
        "\n",
        "def add_gumbel_noise(logits, temperature):\n",
        "    logits = logits.to(torch.float64)\n",
        "    noise = torch.rand_like(logits, dtype=torch.float64).clamp(min=1e-9)\n",
        "    if temperature == 0: return logits\n",
        "    else: gumbel_noise = (-torch.log(noise)) ** temperature; return logits.exp() / (gumbel_noise + 1e-9)\n",
        "\n",
        "def get_num_transfer_tokens_schedule(mask_index, steps):\n",
        "    if steps <= 0: print(f\"Warning: steps is {steps}. Returning empty schedule.\"); return torch.zeros(mask_index.shape[0], 0, device=mask_index.device, dtype=torch.int64)\n",
        "    mask_num = mask_index.sum(dim=1, keepdim=True); steps_tensor = torch.tensor(steps, device=mask_index.device, dtype=torch.long)\n",
        "    safe_steps = torch.where(mask_num.squeeze(1) > 0, steps_tensor, torch.tensor(1, device=mask_index.device, dtype=torch.long))\n",
        "    base = mask_num // safe_steps.unsqueeze(1); remainder = mask_num % safe_steps.unsqueeze(1)\n",
        "    num_transfer_tokens_schedule = (torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base)\n",
        "    for i in range(mask_num.size(0)):\n",
        "        rem = remainder[i].item(); end_idx = min(rem, steps)\n",
        "        if rem > 0: num_transfer_tokens_schedule[i, :end_idx] += 1\n",
        "    total_scheduled = num_transfer_tokens_schedule.sum(dim=1); mismatch_indices = torch.where(total_scheduled != mask_num.squeeze(1))[0]\n",
        "    for i in mismatch_indices:\n",
        "        current_sum = total_scheduled[i].item(); target_sum = mask_num[i, 0].item(); diff = target_sum - current_sum\n",
        "        if diff!=0: print(f\"Correcting schedule mismatch item {i}: diff={diff}\")\n",
        "        step_idx = steps - 1\n",
        "        while diff != 0 and step_idx >= 0:\n",
        "            adjustment = 1 if diff > 0 else -1\n",
        "            if num_transfer_tokens_schedule[i, step_idx].item() + adjustment >= 0: num_transfer_tokens_schedule[i, step_idx] += adjustment; diff -= adjustment\n",
        "            step_idx -= 1\n",
        "        if diff != 0: print(f\"Error: Could not fully correct schedule mismatch item {i}. Final diff: {diff}\")\n",
        "    return num_transfer_tokens_schedule\n",
        "\n",
        "def make_step_rewards(logits, token_masks):\n",
        "    if logits.shape[:-1] != token_masks.shape: print(f\"Error: Logits shape {logits.shape[:-1]} vs mask shape {token_masks.shape}\"); return [[]] * logits.shape[0]\n",
        "    probabilities = F.softmax(logits, dim=-1); probabilities = probabilities * token_masks.unsqueeze(-1)\n",
        "    all_scores_res = []\n",
        "    for i in range(probabilities.size(0)):\n",
        "        sample = probabilities[i]; valid_rows = sample[token_masks[i]]\n",
        "        if valid_rows.numel() > 0:\n",
        "            num_labels = valid_rows.shape[-1]\n",
        "            if num_labels != 2: print(f\"Warning: Expected PRM dim 2, got {num_labels}. Assuming last logit.\"); positive_probs = valid_rows[:, -1]\n",
        "            else: positive_probs = valid_rows.view(-1, 2)[:, 1]\n",
        "            all_scores_res.append(positive_probs.cpu().tolist())\n",
        "        else: all_scores_res.append([])\n",
        "    return all_scores_res\n",
        "\n",
        "def calculate_backmasking_probs(block_scores, backmasking_alpha=5.0, min_prob=0.01):\n",
        "    if not block_scores: return []\n",
        "    scores = np.array(block_scores); inverted_scores = 1.0 - scores; probs = np.exp(backmasking_alpha * inverted_scores)\n",
        "    probs = np.maximum(probs, min_prob); max_p, min_p = probs.max(), probs.min()\n",
        "    if max_p <= min_p: return np.full_like(probs, min_prob)\n",
        "    probs = min_prob + (1 - min_prob) * (probs - min_p) / (max_p - min_p)\n",
        "    return probs\n",
        "\n",
        "def get_backmasking_tokens(block_region_masks, block_probs, backmasking_intensity=0.5, x_shape=None):\n",
        "    if x_shape is None or not block_region_masks: return torch.zeros(x_shape, dtype=torch.bool)\n",
        "    batch_size = block_region_masks[0].shape[0]; seq_len = block_region_masks[0].shape[1]\n",
        "    if len(block_region_masks) != len(block_probs): print(f\"Error: Mismatch block masks ({len(block_region_masks)}) vs probs ({len(block_probs)}).\"); return torch.zeros(x_shape, dtype=torch.bool)\n",
        "    final_mask = torch.zeros(x_shape, dtype=torch.bool, device=block_region_masks[0].device)\n",
        "    for i, (region_mask, prob) in enumerate(zip(block_region_masks, block_probs)):\n",
        "        for b in range(batch_size):\n",
        "            block_token_indices = torch.where(region_mask[b])[0]; block_size = len(block_token_indices);\n",
        "            if block_size == 0: continue\n",
        "            num_to_mask = int(block_size * prob * backmasking_intensity)\n",
        "            if num_to_mask > 0:\n",
        "                perm = torch.randperm(block_size, device=region_mask.device); mask_positions_in_block = block_token_indices[perm[:num_to_mask]]\n",
        "                valid_indices = mask_positions_in_block[mask_positions_in_block < seq_len]; final_mask[b, valid_indices] = True\n",
        "    return final_mask\n",
        "\n",
        "def compute_block_score(block_text, prompt_text, prm_model, prm_tokenizer):\n",
        "    # This function computes score for ONE block\n",
        "    if not block_text.strip(): return 0.0\n",
        "    formatted_block = \"<extra_0>\" + block_text + \"<extra_0>\"; system_prompt = \"Please evaluate the reasoning step provided.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt_text}, {\"role\": \"assistant\", \"content\": formatted_block}]\n",
        "    try: conversation_str = prm_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    except Exception as e: print(f\"Error applying chat template: {e}\"); return 0.0\n",
        "    try:\n",
        "        max_len = 4096\n",
        "        batch = prm_tokenizer(conversation_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
        "        input_ids = batch[\"input_ids\"].to(prm_model.device); attention_mask = batch[\"attention_mask\"].to(prm_model.device)\n",
        "    except Exception as e: print(f\"Error tokenizing for PRM: {e}\"); return 0.0\n",
        "    step_sep_id = prm_tokenizer.convert_tokens_to_ids(\"<extra_0>\"); step_sep_id = step_sep_id[0] if isinstance(step_sep_id, list) else step_sep_id\n",
        "    token_masks = torch.zeros_like(input_ids, dtype=torch.bool)\n",
        "    indices = torch.where(input_ids[0] == step_sep_id)[0]\n",
        "    if len(indices) > 0: token_masks[0, indices[-1]] = True\n",
        "    else: print(\"Warning: <extra_0> token not found for scoring.\"); return 0.0\n",
        "    try:\n",
        "        with torch.no_grad(): outputs = prm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits_or_scores = None\n",
        "        if hasattr(outputs, 'logits'): logits_or_scores = outputs.logits\n",
        "        elif hasattr(outputs, 'scores'): logits_or_scores = outputs.scores\n",
        "        elif hasattr(outputs, 'end_scores'): logits_or_scores = outputs.end_scores\n",
        "        elif isinstance(outputs, torch.Tensor): logits_or_scores = outputs\n",
        "        else: print(f\"Error: Unknown PRM output structure: {type(outputs)}\"); return 0.0\n",
        "        if logits_or_scores is None: print(\"Error: Failed to extract PRM output.\"); return 0.0\n",
        "        # This assumes make_step_rewards can handle the extracted logits_or_scores structure\n",
        "        step_reward = make_step_rewards(logits_or_scores[0:1], token_masks[0:1])\n",
        "        if step_reward and step_reward[0]: avg_score = sum(step_reward[0]) / len(step_reward[0]); return max(0.0, min(1.0, avg_score))\n",
        "        else: return 0.0\n",
        "    except Exception as e: print(f\"Error during PRM inference/score: {e}\"); import traceback; traceback.print_exc(); return 0.0\n",
        "\n",
        "def printable_sequence(x_tensor, tokenizer, mask_id, prompt_len, max_chars=None):\n",
        "    \"\"\"\n",
        "    Render the decoded sequence for the first batch item, replacing every\n",
        "    occurrence of `mask_id` with the literal string “[MASK]”.  The string is\n",
        "    truncated to `max_chars` to keep the log readable.\n",
        "    \"\"\"\n",
        "    toks = x_tensor[0, prompt_len:].tolist()\n",
        "    pieces = []\n",
        "    for t in toks:\n",
        "        if t == mask_id:\n",
        "            pieces.append(\"[MASK]\")\n",
        "        else:\n",
        "            pieces.append(tokenizer.decode([t], skip_special_tokens=True))\n",
        "    txt = \"\".join(pieces)\n",
        "    if max_chars is not None and len(txt) > max_chars:\n",
        "        return txt[:max_chars] + \"...\"\n",
        "    return txt\n",
        "def compute_k_block_scores_batched(\n",
        "    x,\n",
        "    start_block_index,       # index of first block in window\n",
        "    num_blocks_in_window,    # number of blocks to score (K)\n",
        "    prompt_len,\n",
        "    block_length,\n",
        "    prompt_text,             # text of the initial prompt\n",
        "    tokenizer,\n",
        "    prm_model,\n",
        "    prm_tokenizer\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute scores for K blocks in one batched forward pass.\n",
        "    \"\"\"\n",
        "    # 1. Extract each block's text\n",
        "    block_texts = []\n",
        "    for i in range(num_blocks_in_window):\n",
        "        idx = start_block_index + i\n",
        "        start = prompt_len + idx * block_length\n",
        "        end = start + block_length\n",
        "        block_text = tokenizer.decode(x[0, start:end], skip_special_tokens=True)\n",
        "        block_texts.append(block_text)\n",
        "\n",
        "    # 2. Build one conversation string per block\n",
        "    convs = []\n",
        "    system_msg = {\"role\": \"system\", \"content\": \"Please evaluate the reasoning step provided.\"}\n",
        "    user_msg = {\"role\": \"user\",   \"content\": prompt_text}\n",
        "    for block_text in block_texts:\n",
        "        assistant_msg = {\"role\": \"assistant\", \"content\": f\"<extra_0>{block_text}<extra_0>\"}\n",
        "        # Apply chat template for each individually (tokenize later as batch)\n",
        "        conv_str = prm_tokenizer.apply_chat_template(\n",
        "            [system_msg, user_msg, assistant_msg],\n",
        "            add_generation_prompt=False,\n",
        "            tokenize=False\n",
        "        )\n",
        "        convs.append(conv_str)\n",
        "\n",
        "    # 3. Tokenize all conversations at once (padding/truncation ensures uniform shape)\n",
        "    batch = prm_tokenizer(\n",
        "        convs,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=4096\n",
        "    )\n",
        "    input_ids = batch[\"input_ids\"].to(prm_model.device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(prm_model.device)\n",
        "\n",
        "    # 4. Forward pass for all K blocks in a single call\n",
        "    with torch.no_grad():\n",
        "        outputs = prm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = None\n",
        "        if hasattr(outputs, \"logits\"):\n",
        "            logits = outputs.logits\n",
        "        elif hasattr(outputs, \"scores\"):\n",
        "            logits = outputs.scores\n",
        "        elif hasattr(outputs, \"end_scores\"):\n",
        "            logits = outputs.end_scores\n",
        "        else:\n",
        "            logits = outputs if isinstance(outputs, torch.Tensor) else None\n",
        "\n",
        "    # 5. Build token masks: mark <extra_0> positions for each example\n",
        "    sep_id = prm_tokenizer.convert_tokens_to_ids(\"<extra_0>\")\n",
        "    # If sep_id is list, take first\n",
        "    sep_id = sep_id[0] if isinstance(sep_id, (list, tuple)) else sep_id\n",
        "    token_masks = torch.zeros_like(input_ids, dtype=torch.bool)\n",
        "    for b in range(input_ids.size(0)):\n",
        "        idxs = (input_ids[b] == sep_id).nonzero(as_tuple=True)[0]\n",
        "        if idxs.numel() > 0:\n",
        "            token_masks[b, idxs[-1]] = True\n",
        "\n",
        "    # 6. Compute step rewards in batch\n",
        "    batch_rewards = make_step_rewards(logits, token_masks)  # returns list of lists\n",
        "\n",
        "    # 7. Average rewards per block to get a score\n",
        "    scores = []\n",
        "    for rewards in batch_rewards:\n",
        "        if rewards:\n",
        "            avg = sum(rewards) / len(rewards)\n",
        "            scores.append(float(max(0.0, min(1.0, avg))))\n",
        "        else:\n",
        "            scores.append(0.0)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def demask_steps_refactored(x, mask_schedule, limit_mask, model, temperature, cfg_scale=0.0, remasking=\"low_confidence\", mask_id=126336):\n",
        "    steps = mask_schedule.shape[1]; prompt_len = 0; initial_non_masked = (x != mask_id);\n",
        "    if initial_non_masked[0,0]:\n",
        "        non_masked_indices = torch.where(initial_non_masked[0])[0]\n",
        "        if len(non_masked_indices) > 0 and torch.all(non_masked_indices == torch.arange(len(non_masked_indices), device=x.device)): prompt_len = len(non_masked_indices)\n",
        "    for i in range(steps):\n",
        "        num_to_transfer_this_step = mask_schedule[:, i];\n",
        "        if torch.all(num_to_transfer_this_step == 0): continue\n",
        "        current_mask_index = (x == mask_id) & limit_mask\n",
        "        if not current_mask_index.any(): break\n",
        "        try:\n",
        "            if cfg_scale > 0.0:\n",
        "                cond_x = x.clone(); un_x = torch.full_like(x, mask_id)\n",
        "                if prompt_len > 0: un_x[:, :prompt_len] = x[:, :prompt_len]\n",
        "                x_in = torch.cat([cond_x, un_x], dim=0); logits_full = model(x_in).logits\n",
        "                logits, un_logits = torch.chunk(logits_full, 2, dim=0); logits = logits + cfg_scale * (logits - un_logits)\n",
        "            else: logits = model(x).logits\n",
        "        except Exception as e: print(f\"Error during model inference: {e}\"); break\n",
        "        if temperature == 0: x0 = torch.argmax(logits, dim=-1)\n",
        "        else:\n",
        "             logits_with_noise = add_gumbel_noise(logits, temperature)\n",
        "             if torch.isinf(logits_with_noise).any() or torch.isnan(logits_with_noise).any(): print(f\"Warning: NaN/Inf noisy logits step {i}. Using argmax.\"); x0 = torch.argmax(logits, dim=-1)\n",
        "             else: x0 = torch.argmax(logits_with_noise, dim=-1)\n",
        "        if remasking == \"low_confidence\": p = F.softmax(logits.to(torch.float64), dim=-1); x0_clamped = x0.clamp(0, p.shape[-1] - 1); x0_p = torch.gather(p, dim=-1, index=x0_clamped.unsqueeze(-1)).squeeze(-1)\n",
        "        elif remasking == \"random\": x0_p = torch.rand_like(x0, dtype=torch.float64)\n",
        "        else: raise NotImplementedError(remasking)\n",
        "        confidence_for_selection = torch.where(current_mask_index, x0_p, torch.tensor(-np.inf, dtype=x0_p.dtype, device=x.device))\n",
        "        batch_size = x.shape[0]\n",
        "        for j in range(batch_size):\n",
        "            num_to_transfer = num_to_transfer_this_step[j].item()\n",
        "            if num_to_transfer > 0:\n",
        "                available_masked = current_mask_index[j].sum().item(); k = min(num_to_transfer, available_masked)\n",
        "                if k > 0:\n",
        "                    try:\n",
        "                        conf_j = confidence_for_selection[j]; select_indices = None\n",
        "                        if conf_j.dim() == 0:\n",
        "                           if conf_j > -np.inf: select_indices = torch.where(current_mask_index[j])[0]\n",
        "                        elif k > conf_j.shape[0]: k = conf_j.shape[0]\n",
        "                        if k > 0 and select_indices is None: _, select_indices = torch.topk(conf_j, k=k, largest=True)\n",
        "                        if select_indices is not None and len(select_indices) > 0: x[j, select_indices] = x0[j, select_indices]\n",
        "                    except RuntimeError as e: print(f\"Error in topk item {j}: {e}\"); continue\n",
        "# --- End Helper Functions ---\n",
        "\n",
        "\n",
        "# --- NEW HELPER: Compute Scores for a Window of K Blocks ---\n",
        "def compute_k_block_scores(\n",
        "    x,\n",
        "    start_block_index, # The index of the first block in the window (0-based)\n",
        "    num_blocks_in_window,\n",
        "    prompt_len,\n",
        "    block_length,\n",
        "    prompt_text, # Assumes prompt_text is for batch item 0\n",
        "    tokenizer,\n",
        "    prm_model,\n",
        "    prm_tokenizer\n",
        "):\n",
        "    \"\"\"Computes scores for a specific window of K blocks.\"\"\"\n",
        "    scores = []\n",
        "    print(f\"Scoring blocks {start_block_index + 1} to {start_block_index + num_blocks_in_window}...\")\n",
        "    for i in range(num_blocks_in_window):\n",
        "        block_idx = start_block_index + i\n",
        "        b_start = prompt_len + block_idx * block_length\n",
        "        b_end = b_start + block_length\n",
        "\n",
        "        # Check bounds for the first batch item (scoring assumes batch size 1)\n",
        "        if b_end > x.shape[1]:\n",
        "             print(f\"Warning: Block index {block_idx+1} exceeds sequence length during scoring.\")\n",
        "             scores.append(0.0) # Append default score\n",
        "             continue\n",
        "\n",
        "        # Decode only the first batch item for scoring\n",
        "        block_text = tokenizer.decode(x[0, b_start:b_end], skip_special_tokens=True)\n",
        "        block_score = compute_block_score(block_text, prompt_text, prm_model, prm_tokenizer)\n",
        "        scores.append(block_score)\n",
        "    return scores\n",
        "# --- End Helper Functions ---\n",
        "\n",
        "# --- NEW HELPER: Batched Scoring for Multiple Samples ---\n",
        "def compute_k_block_scores_multi_sample(\n",
        "    x_batch,\n",
        "    start_block_index,\n",
        "    num_blocks_in_window,\n",
        "    prompt_len,\n",
        "    block_length,\n",
        "    prompt_texts,\n",
        "    tokenizer,\n",
        "    prm_model,\n",
        "    prm_tokenizer,\n",
        "    original_batch_size,\n",
        "):\n",
        "    \"\"\"Return shape [B][N][K].  If PRM evaluation fails, supply random scores.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # -- existing implementation kept verbatim --------------------------------\n",
        "        num_total_samples = x_batch.shape[0]\n",
        "        if num_total_samples % original_batch_size != 0:\n",
        "            raise RuntimeError(\"sample count mismatch with batch size\")\n",
        "\n",
        "        num_samples_per_item = num_total_samples // original_batch_size\n",
        "        # Build chat prompts ------------------------------------------------------\n",
        "        system_msg = {\"role\": \"system\", \"content\": \"Please evaluate the reasoning step provided.\"}\n",
        "        all_convs = []\n",
        "        for b_orig in range(original_batch_size):\n",
        "            user_msg = {\"role\": \"user\", \"content\": prompt_texts[b_orig]}\n",
        "            for s in range(num_samples_per_item):\n",
        "                for k in range(num_blocks_in_window):\n",
        "                    blk = start_block_index + k\n",
        "                    s_idx = b_orig * num_samples_per_item + s\n",
        "                    b_start = prompt_len + blk * block_length\n",
        "                    b_end   = b_start + block_length\n",
        "                    block_text = tokenizer.decode(x_batch[s_idx, b_start:b_end], skip_special_tokens=True)\n",
        "                    assistant_msg = {\"role\": \"assistant\", \"content\": f\"<extra_0>{block_text}<extra_0>\"}\n",
        "                    all_convs.append(prm_tokenizer.apply_chat_template(\n",
        "                        [system_msg, user_msg, assistant_msg],\n",
        "                        add_generation_prompt=False,\n",
        "                        tokenize=False,\n",
        "                    ))\n",
        "        if not all_convs:\n",
        "            raise RuntimeError(\"no conversations built for PRM evaluation\")\n",
        "\n",
        "        batch_tokens = prm_tokenizer(\n",
        "            all_convs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=4096,\n",
        "        )\n",
        "        input_ids      = batch_tokens[\"input_ids\"].to(prm_model.device)\n",
        "        attention_mask = batch_tokens[\"attention_mask\"].to(prm_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = prm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "\n",
        "        # build token-masks -------------------------------------------------------\n",
        "        sep_id = prm_tokenizer.convert_tokens_to_ids(\"<extra_0>\")\n",
        "        if isinstance(sep_id, (list, tuple)):\n",
        "            sep_id = sep_id[0]\n",
        "        token_masks = (input_ids == sep_id)\n",
        "        token_masks = token_masks.scatter(-1, (token_masks.cumsum(-1) == 1).long(), token_masks)  # last occurrence\n",
        "\n",
        "        rewards = make_step_rewards(logits, token_masks)\n",
        "\n",
        "        # reshape to [B][N][K] ----------------------------------------------------\n",
        "        out = [[[] for _ in range(num_samples_per_item)] for _ in range(original_batch_size)]\n",
        "        idx = 0\n",
        "        for b in range(original_batch_size):\n",
        "            for s in range(num_samples_per_item):\n",
        "                sample_scores = []\n",
        "                for _ in range(num_blocks_in_window):\n",
        "                    sample_scores.append(rewards[idx][0] if rewards[idx] else 0.0)\n",
        "                    idx += 1\n",
        "                out[b][s] = sample_scores\n",
        "        return out\n",
        "\n",
        "    except Exception as e:\n",
        "        # --------------------------------------------------------\n",
        "        # New fallback path: apologise and assign random scores.\n",
        "        # --------------------------------------------------------\n",
        "        print(\"\\n[PRM ERROR] Apologies! The reward model could not be evaluated on this hardware (\" + str(e) + \").\"\n",
        "              \" Falling back to random block scores so generation can continue.\\n\")\n",
        "        B = x_batch.shape[0] // original_batch_size or 1  # samples per item\n",
        "        K = num_blocks_in_window\n",
        "        # Return random scores in expected nested list structure\n",
        "        return [[_random_block_scores(K) for _ in range(B)] for __ in range(original_batch_size)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_prm_window_score(\n",
        "    model,\n",
        "    prompt,\n",
        "    prm_model,\n",
        "    tokenizer,\n",
        "    prm_tokenizer,\n",
        "    steps=128,\n",
        "    gen_length=512,\n",
        "    block_length=32,\n",
        "    temperature=0.3,\n",
        "    cfg_scale=0.0,\n",
        "    remasking=\"low_confidence\",\n",
        "    mask_id=126336,\n",
        "    backmasking_alpha=5.0,\n",
        "    backmasking_intensity=0.5,\n",
        "    backmasking_frequency=8, # K value\n",
        "    backmasking_threshold=0.8,\n",
        "    num_refinement_samples=12, # N value (replaces max_retry_attempts)\n",
        "    selection_metric=\"product\" # \"product\" or \"min_score\"\n",
        "):\n",
        "    print(\"===== Generation Started (PRM Window Score + Batch Refine) =====\")\n",
        "    if not isinstance(prompt, torch.Tensor) or prompt.dim() != 2: print(\"Error: Prompt must be 2D Tensor.\"); return None\n",
        "    original_batch_size = prompt.shape[0]\n",
        "    if gen_length <= 0 or block_length <= 0 or steps <= 0 or backmasking_frequency <= 0 or num_refinement_samples <= 0: print(\"Error: lengths, steps, frequency, samples > 0.\"); return None\n",
        "    if gen_length % block_length != 0: print(f\"Warning: gen_length not divisible by block_length.\")\n",
        "    num_blocks = gen_length // block_length; effective_gen_length = num_blocks * block_length\n",
        "    if steps < num_blocks: print(f\"Warning: steps ({steps}) < num_blocks ({num_blocks}).\")\n",
        "    block_steps_base = steps // num_blocks; remainder_steps = steps % num_blocks\n",
        "    print(f\"Batch Size: {original_batch_size}\")\n",
        "    print(f\"Prompt (Item 0): {tokenizer.decode(prompt[0], skip_special_tokens=True)[:100]}...\")\n",
        "    print(f\"Config: Steps={steps}, GenLen={effective_gen_length}, BlockLen={block_length}, K={backmasking_frequency}, Thresh={backmasking_threshold}, N_Samples={num_refinement_samples}, SelectMetric={selection_metric}\")\n",
        "\n",
        "    device = model.device; prompt_len = prompt.shape[1]\n",
        "    x = torch.full((original_batch_size, prompt_len + effective_gen_length), mask_id, dtype=torch.long, device=device)\n",
        "    x[:, : prompt_len] = prompt.clone()\n",
        "    K = backmasking_frequency\n",
        "    N = num_refinement_samples\n",
        "\n",
        "    # Decode prompts for all items in the original batch\n",
        "    prompt_texts = [tokenizer.decode(p, skip_special_tokens=True) for p in prompt]\n",
        "\n",
        "    # Store scores per original batch item\n",
        "    block_scores_all = [[0.0] * num_blocks for _ in range(original_batch_size)]\n",
        "    # Store region masks (only need one set, applied to all samples)\n",
        "    block_masks_list = []\n",
        "\n",
        "    # --- Main Generation Loop ---\n",
        "    for num_block in range(num_blocks):\n",
        "        print(f\"\\n--- Generating Block {num_block+1}/{num_blocks} ---\")\n",
        "        block_start_idx = prompt_len + num_block * block_length\n",
        "        block_end_idx = block_start_idx + block_length\n",
        "\n",
        "        # --- 1. Generate Block Content (for the current state 'x') ---\n",
        "        current_block_mask_bool = (x[:, block_start_idx:block_end_idx] == mask_id)\n",
        "        if current_block_mask_bool.any():\n",
        "            steps_this_block = block_steps_base + 1 if num_block < remainder_steps else block_steps_base\n",
        "            if steps_this_block > 0:\n",
        "                # Create mask schedule only for the portion to be generated\n",
        "                schedule_mask_shape = torch.zeros_like(x, dtype=torch.bool)\n",
        "                schedule_mask_shape[:, block_start_idx:block_end_idx] = current_block_mask_bool\n",
        "                mask_schedule_block = get_num_transfer_tokens_schedule(schedule_mask_shape, steps_this_block)\n",
        "\n",
        "                # Limit demasking to the current block for initial generation\n",
        "                limit_mask_block = torch.zeros_like(x, dtype=torch.bool)\n",
        "                limit_mask_block[:, :block_end_idx] = True # Allow attending to previous blocks\n",
        "\n",
        "                # Apply demasking to the current state 'x'\n",
        "                demask_steps_refactored(x, mask_schedule_block, limit_mask_block, model, temperature, cfg_scale, remasking, mask_id)\n",
        "            else: print(f\"Warning: 0 steps for block {num_block+1}. Skipping generation.\")\n",
        "        else: print(f\"Block {num_block+1} already filled. Skipping generation.\")\n",
        "\n",
        "        # Store region mask for this block (shape: B, L)\n",
        "        block_region_mask = torch.zeros_like(x, dtype=torch.bool)\n",
        "        block_region_mask[:, block_start_idx:block_end_idx] = True\n",
        "        block_masks_list.append(block_region_mask)\n",
        "\n",
        "        # --- 2. Score Window & Potential Batch Refinement (Every K blocks) ---\n",
        "        is_check_time = (num_block + 1) % K == 0 and K > 0\n",
        "        if is_check_time:\n",
        "            start_block_idx_window = num_block - K + 1\n",
        "            print(f\"\\n--- Evaluating Window: Blocks {start_block_idx_window + 1} to {num_block + 1} ---\")\n",
        "\n",
        "            # --- Score the K blocks for the CURRENT state 'x' (original batch size) ---\n",
        "            # Use the original batched scoring function here for the initial check\n",
        "            current_window_scores_per_item = []\n",
        "            # TODO: Optimize this initial scoring - can we reuse compute_k_block_scores_multi_sample?\n",
        "            # For now, score each item separately or adapt compute_k_block_scores_batched for B>1\n",
        "            print(\"Scoring current state (before potential refinement)...\")\n",
        "            temp_scores_container = compute_k_block_scores_multi_sample(\n",
        "                 x, start_block_idx_window, K, prompt_len, block_length,\n",
        "                 prompt_texts, tokenizer, prm_model, prm_tokenizer, original_batch_size\n",
        "            )\n",
        "            # temp_scores_container is List[List[List[float]]] with inner list size 1\n",
        "            current_window_scores_per_item = [item_scores[0] for item_scores in temp_scores_container] # Extract the single sample scores\n",
        "\n",
        "\n",
        "            # Update the main scores list for all batch items based on current state\n",
        "            for b in range(original_batch_size):\n",
        "                if current_window_scores_per_item and b < len(current_window_scores_per_item):\n",
        "                     scores_b = current_window_scores_per_item[b]\n",
        "                     if len(scores_b) == K:\n",
        "                          for i in range(K):\n",
        "                              block_scores_all[b][start_block_idx_window + i] = scores_b[i]\n",
        "                     else: print(f\"Warning: Incorrect score count for item {b} pre-refine.\")\n",
        "\n",
        "\n",
        "            # --- Check Threshold and Trigger Batch Refinement ---\n",
        "            needs_refinement_flags = [False] * original_batch_size\n",
        "            for b in range(original_batch_size):\n",
        "                 if current_window_scores_per_item and b < len(current_window_scores_per_item):\n",
        "                    min_score_b = min(current_window_scores_per_item[b]) if current_window_scores_per_item[b] else 0.0\n",
        "                    if min_score_b < backmasking_threshold:\n",
        "                        needs_refinement_flags[b] = True\n",
        "                        print(f\"Item {b}: Min score {min_score_b:.4f} < threshold ({backmasking_threshold:.4f}). Needs refinement.\")\n",
        "                    else:\n",
        "                        print(f\"Item {b}: Min score {min_score_b:.4f} >= threshold. OK.\")\n",
        "                 else: print(f\"Warning: Missing scores for item {b}, cannot check threshold.\")\n",
        "\n",
        "\n",
        "            if any(needs_refinement_flags):\n",
        "                print(f\"--- Starting Batch Refinement for {sum(needs_refinement_flags)} items ---\")\n",
        "\n",
        "                # 1. Calculate backmasking probs based on *initial* scores (per item)\n",
        "                backmasking_probs_per_item = []\n",
        "                for b in range(original_batch_size):\n",
        "                     scores_b = current_window_scores_per_item[b] if current_window_scores_per_item else []\n",
        "                     # Use scores if needed refinement, otherwise dummy probs (won't be used)\n",
        "                     probs_b = calculate_backmasking_probs(scores_b, backmasking_alpha) if needs_refinement_flags[b] else [0.0] * K\n",
        "                     backmasking_probs_per_item.append(probs_b)\n",
        "\n",
        "                # 2. Get backmasking tokens for the window (for the whole original batch)\n",
        "                window_block_masks = block_masks_list[start_block_idx_window : num_block + 1] # List of K tensors (B, L)\n",
        "\n",
        "                # Need to generate the mask based on per-item probabilities\n",
        "                # get_backmasking_tokens expects List[Tensor(B,L)], List[List[float]]? No, probs is flat list.\n",
        "                # Let's adapt: create one mask for the whole batch B, applying intensity only where needed.\n",
        "                full_backmasking_mask = torch.zeros_like(x, dtype=torch.bool) # Shape (B, L)\n",
        "                for b_idx in range(original_batch_size):\n",
        "                    if needs_refinement_flags[b_idx]:\n",
        "                         # Generate mask only for this item b_idx using its probs\n",
        "                         item_mask = get_backmasking_tokens(\n",
        "                              [bm[b_idx:b_idx+1] for bm in window_block_masks], # Slice masks for this item\n",
        "                              backmasking_probs_per_item[b_idx],\n",
        "                              backmasking_intensity,\n",
        "                              x_shape=(1, x.shape[1]) # Shape for single item\n",
        "                         )\n",
        "                         # Apply this item's mask to the full batch mask\n",
        "                         if item_mask is not None and item_mask.shape[0] == 1:\n",
        "                              full_backmasking_mask[b_idx] = item_mask[0]\n",
        "\n",
        "                num_backmasked = full_backmasking_mask.sum().item()\n",
        "                if num_backmasked == 0:\n",
        "                    print(\"No tokens selected for backmasking across batch. Skipping refinement.\")\n",
        "                    continue # Skip to next block generation phase\n",
        "\n",
        "                print(f\"Total tokens selected for backmasking across batch: {num_backmasked}\")\n",
        "\n",
        "                # 3. Create Batch for Refinement (N samples per original item)\n",
        "                # repeat_interleave duplicates items sequentially: [item0_s0, item0_s1, ..., item1_s0, ...]\n",
        "                x_batch = x.repeat_interleave(N, dim=0) # Shape (B*N, L)\n",
        "                mask_batch = full_backmasking_mask.repeat_interleave(N, dim=0) # Shape (B*N, L)\n",
        "\n",
        "                # 4. Apply Mask to Batch\n",
        "                x_batch[mask_batch] = mask_id\n",
        "\n",
        "                # 5. Batched Refinement (Demasking)\n",
        "                print(f\"Running batched refinement for {x_batch.shape[0]} samples...\")\n",
        "                # Limit refinement to the full sequence length (prompt+gen)\n",
        "                refinement_mask_idx = (x_batch == mask_id) & (torch.arange(x_batch.shape[1], device=device).unsqueeze(0) >= prompt_len)\n",
        "                # Use total number of steps = total masked tokens for simplicity (might be slow)\n",
        "                # Alternative: fixed number of steps? Let's stick to full for now.\n",
        "                # Calculate schedule for the large batch\n",
        "                total_masked_in_batch = refinement_mask_idx.sum()\n",
        "                # Ensure steps > 0 if there are masked tokens\n",
        "                refinement_steps = max(1, total_masked_in_batch.item() // x_batch.shape[0]) if total_masked_in_batch > 0 else 0 # Avg steps per sample\n",
        "                refinement_steps = min(refinement_steps, 128)\n",
        "                print(f\"Refinement using approx {refinement_steps} steps per sample.\")\n",
        "\n",
        "                if refinement_steps > 0:\n",
        "                    mask_schedule_refine = get_num_transfer_tokens_schedule(refinement_mask_idx, refinement_steps)\n",
        "                    window_token_start = prompt_len + start_block_idx_window * block_length\n",
        "                    # The end index should be exclusive, covering up to the end of the *last* block in the window\n",
        "                    window_token_end = prompt_len + (num_block + 1) * block_length # num_block is the *last* block index\n",
        "\n",
        "                    # Limit refinement changes strictly to the current window\n",
        "                    limit_mask_refine = torch.zeros_like(x_batch, dtype=torch.bool)\n",
        "                    limit_mask_refine[:, prompt_len:window_token_end] = True # Corrected limit\n",
        "\n",
        "                    print(f\"Refinement limit mask set for tokens {window_token_start} to {window_token_end}\") # Add logging\n",
        "\n",
        "                    demask_steps_refactored(\n",
        "                            x_batch,\n",
        "                            mask_schedule_refine,\n",
        "                            limit_mask_refine,\n",
        "                        model,\n",
        "                        temperature,\n",
        "                        cfg_scale,\n",
        "                        remasking,\n",
        "                        mask_id,\n",
        "                    )\n",
        "\n",
        "                    print(f\"Refined {int(refinement_mask_idx.sum().item())} tokens over \"\n",
        "                            f\"{refinement_steps} steps for {x_batch.size(0)} samples.\")\n",
        "                else:\n",
        "                    print(\"No refinement steps needed (or no tokens masked).\")\n",
        "\n",
        "\n",
        "                # 6. Batched Scoring of Refined Samples\n",
        "                print(\"Scoring refined samples...\")\n",
        "                # all_refined_scores shape: List[List[List[float]]] -> [original_batch][sample][K scores]\n",
        "                all_refined_scores = compute_k_block_scores_multi_sample(\n",
        "                    x_batch, start_block_idx_window, K, prompt_len, block_length,\n",
        "                    prompt_texts, tokenizer, prm_model, prm_tokenizer, original_batch_size\n",
        "                )\n",
        "\n",
        "                # 7. Select Best Sample for each original batch item\n",
        "                print(\"Selecting best sample for each original batch item...\")\n",
        "                new_x = torch.zeros_like(x) # Create new tensor for the chosen states\n",
        "                best_scores_chosen = [[0.0]*K for _ in range(original_batch_size)] # Store scores of chosen samples\n",
        "\n",
        "                for b in range(original_batch_size):\n",
        "                    if not needs_refinement_flags[b]:\n",
        "                        # If this item didn't need refinement, keep original state and scores\n",
        "                        new_x[b] = x[b]\n",
        "                        best_scores_chosen[b] = block_scores_all[b][start_block_idx_window : start_block_idx_window + K]\n",
        "                        continue\n",
        "\n",
        "                    # This item needed refinement, find the best among its N samples\n",
        "                    best_sample_idx_local = -1\n",
        "                    best_metric_val = -float('inf')\n",
        "                    valid_samples_found = False\n",
        "\n",
        "                    if b < len(all_refined_scores):\n",
        "                        samples_for_item_b = all_refined_scores[b] # List of N lists (K scores each)\n",
        "                        for s_idx, sample_scores in enumerate(samples_for_item_b):\n",
        "                            if not sample_scores: continue # Skip if scoring failed for this sample\n",
        "\n",
        "                            current_metric_val = 0.0\n",
        "                            if selection_metric == \"product\":\n",
        "                                # Use small epsilon to avoid issues with zero scores\n",
        "                                current_metric_val = math.prod(max(s, 1e-9) for s in sample_scores)\n",
        "                            elif selection_metric == \"min_score\":\n",
        "                                current_metric_val = min(sample_scores)\n",
        "                            else: # Default to product\n",
        "                                current_metric_val = math.prod(max(s, 1e-9) for s in sample_scores)\n",
        "\n",
        "                            if current_metric_val > best_metric_val:\n",
        "                                best_metric_val = current_metric_val\n",
        "                                best_sample_idx_local = s_idx\n",
        "                                valid_samples_found = True\n",
        "                    else: print(f\"Warning: Missing scores for original batch item {b} during selection.\")\n",
        "\n",
        "\n",
        "                    if valid_samples_found:\n",
        "                        best_sample_idx_global = b * N + best_sample_idx_local\n",
        "                        new_x[b] = x_batch[best_sample_idx_global]\n",
        "                        best_scores_chosen[b] = all_refined_scores[b][best_sample_idx_local]\n",
        "                        print(f\"Item {b}: Chose sample {best_sample_idx_local} (Metric: {best_metric_val:.6f}, Scores: {[f'{s:.3f}' for s in best_scores_chosen[b]]})\")\n",
        "                    else:\n",
        "                        # Fallback: Keep original if no valid refined sample found\n",
        "                        print(f\"Item {b}: No valid refined samples found. Keeping original state.\")\n",
        "                        new_x[b] = x[b]\n",
        "                        best_scores_chosen[b] = block_scores_all[b][start_block_idx_window : start_block_idx_window + K]\n",
        "\n",
        "\n",
        "                # 8. Update Overall State and Scores\n",
        "                x = new_x # Update the main state tensor 'x'\n",
        "                # Update the main scores list with the chosen scores for the window\n",
        "                for b in range(original_batch_size):\n",
        "                    for i in range(K):\n",
        "                         block_scores_all[b][start_block_idx_window + i] = best_scores_chosen[b][i]\n",
        "\n",
        "            # Else (if not any(needs_refinement_flags)): No refinement needed, scores already updated\n",
        "\n",
        "\n",
        "    # --- Handle Final Partial Window (Scoring Only) ---\n",
        "    remaining_blocks = num_blocks % K\n",
        "    if remaining_blocks > 0 and num_blocks > 0:\n",
        "        start_block_idx_final = num_blocks - remaining_blocks\n",
        "        print(f\"\\n--- Scoring Final Partial Window: Blocks {start_block_idx_final + 1} to {num_blocks} ---\")\n",
        "        # Use the multi-sample scorer (with N=1 effectively, as we score the final 'x')\n",
        "        final_scores_container = compute_k_block_scores_multi_sample(\n",
        "             x, start_block_idx_final, remaining_blocks, prompt_len, block_length,\n",
        "             prompt_texts, tokenizer, prm_model, prm_tokenizer, original_batch_size\n",
        "        )\n",
        "        final_window_scores_per_item = [item_scores[0] for item_scores in final_scores_container]\n",
        "\n",
        "\n",
        "        # Update the main scores list for the final blocks\n",
        "        for b in range(original_batch_size):\n",
        "            if final_window_scores_per_item and b < len(final_window_scores_per_item):\n",
        "                 scores_b = final_window_scores_per_item[b]\n",
        "                 if len(scores_b) == remaining_blocks:\n",
        "                     for i in range(remaining_blocks):\n",
        "                         block_scores_all[b][start_block_idx_final + i] = scores_b[i]\n",
        "                     print(f\"Item {b} Final Window Scores: {[f'{s:.4f}' for s in scores_b]}\")\n",
        "                 else: print(f\"Warning: Incorrect score count for item {b} in final window.\")\n",
        "            else: print(f\"Warning: Missing final scores for item {b}.\")\n",
        "\n",
        "\n",
        "    # --- Final Output ---\n",
        "    print(\"\\n===== Generation Complete =====\")\n",
        "    for b in range(original_batch_size):\n",
        "         final_masked_count = (x[b, prompt_len:] == mask_id).sum().item()\n",
        "         if final_masked_count > 0: print(f\"Warning (Item {b}): {final_masked_count} mask tokens remain.\")\n",
        "         print(f\"Final block scores (Item {b}): {[f'{s:.3f}' for s in block_scores_all[b]]}\")\n",
        "         decoded = tokenizer.decode(x[b, prompt_len:], skip_special_tokens=True)\n",
        "         print(f\"\\nGenerated output (Item {b}, first 500 chars):\\n{decoded[:500]}\\n{'...' if len(decoded) > 500 else ''}\")\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "a2bc0b92856e44de9c181de2cb290022",
            "d4fe9e413cc042a1840f7876f5b03a49",
            "a3693b4fd01048ba832f9f676d8296f7",
            "5c1b253d63594533b0a63d7bf0c21862",
            "f8f40c7bbb664c628b4982ae35e2656e",
            "68eabe4bc4c54b41b51a4d37a039357e",
            "07501010fa4c4b6380a9fea237562844",
            "8ad4f22e34c042d1afea297126e6dfe9",
            "a955f2cdd4d147938cbc979ef52b94df",
            "b8e4fecf8d224b8b9b3593124f8d6b4a",
            "f2f64b46a77a491d9a15d9d3ab60f0b2",
            "836f24ccdae549a2bed4e8169025bc22",
            "c534cf84f04a460680539df7da7c704e",
            "b9f2e5f1b9824be88d0938e37de712fa",
            "51ec0c7c456342b7b42de193640f14f5",
            "7071a4f47d8949e8bf26ed9570502608",
            "068cb134405d43c2830c6e0e87f55c31"
          ]
        },
        "id": "CCoBlWYUhWT6",
        "outputId": "a54f4057-2d9e-49df-e0a8-e8ff03dee6d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2bc0b92856e44de9c181de2cb290022",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import traceback\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "# os.environ[\"HF_TOKEN\"] = YOUR_HF_TOKEN       # ← paste token\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "\n",
        "\n",
        "import functools\n",
        "print = functools.partial(print, flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"FunAGI/LLaDA-8B-Instruct-gptqmodel-4bit\"\n",
        "PRM_MODEL_NAME = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
        "MAX_QUESTIONS = None\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "GEN_PARAMS_SHARED = {\n",
        "    \"steps\": 128,\n",
        "    \"gen_length\": 512,\n",
        "    \"block_length\": 32,\n",
        "    \"cfg_scale\": 0.0,\n",
        "    \"remasking\": \"low_confidence\",\n",
        "}\n",
        "GEN_PARAMS_RAW = {\"temperature\": 0.8}\n",
        "GEN_PARAMS_PRM = {\"temperature\": 0.8}\n",
        "GEN_PARAMS_BM = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"backmasking_frequency\": 8,           # How often to apply backmasking (every N blocks)\n",
        "    \"backmasking_threshold\": 0.8,         # Score threshold under which backmasking triggers\n",
        "    \"backmasking_intensity\": 0.8,         # Proportion of tokens to backmask\n",
        "    \"num_refinement_samples\": 5,              # Retry block N times until it reaches threshold\n",
        "    \"backmasking_alpha\": 10.0,             # Controls how aggressive score→prob mapping is\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_single_prompt(question, model, tokenizer, prm_model, prm_tokenizer, ground_truth=None):\n",
        "    print(f\"\\nEvaluating Single Question:\")\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    try:\n",
        "        prompt_input = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": question}],\n",
        "            add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "        input_ids = tokenizer(prompt_input, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR (Prompt Formatting): {e}\")\n",
        "        return\n",
        "\n",
        "    current_q_results = {}\n",
        "\n",
        "    # --- Backmasking ---\n",
        "    print(\"\\nRunning Backmasking...\")\n",
        "    start = time.time()\n",
        "    try:\n",
        "        bm_out = generate_prm_window_score(\n",
        "            model=model,\n",
        "            prompt=input_ids,\n",
        "            prm_model=prm_model,\n",
        "            tokenizer=tokenizer,\n",
        "            prm_tokenizer=prm_tokenizer,\n",
        "            **GEN_PARAMS_SHARED,\n",
        "            **GEN_PARAMS_BM,\n",
        "        )\n",
        "        bm_response = tokenizer.decode(\n",
        "            bm_out[0, input_ids.shape[1]:], skip_special_tokens=True\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "\n",
        "        print(f\"\\n--- Backmasking Output ---\\n{bm_response.strip()}\\n\")\n",
        "        print(f\"Backmasking Time: {elapsed:.2f}s\")\n",
        "\n",
        "        current_q_results.update({\n",
        "            \"response\": bm_response,\n",
        "            \"time\": elapsed\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Backmasking:\\n{traceback.format_exc()}\")\n",
        "\n",
        "    return current_q_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9EXFMf1o-vZ",
        "outputId": "23359d8f-4a72-4944-d504-3a960d9aefe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gptqmodel in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: accelerate>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (1.6.0)\n",
            "Requirement already satisfied: datasets>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (4.51.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (3.6.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (24.2)\n",
            "Requirement already satisfied: device-smi==0.4.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=5.29.3 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (5.29.4)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (11.2.1)\n",
            "Requirement already satisfied: hf-transfer>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.1.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.31.2)\n",
            "Requirement already satisfied: random-word==1.0.13 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (1.0.13)\n",
            "Requirement already satisfied: tokenicer==0.0.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.0.4)\n",
            "Requirement already satisfied: logbar==0.0.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.0.4)\n",
            "Requirement already satisfied: autopep8<3.0.0,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from random-word==1.0.13->gptqmodel) (2.3.2)\n",
            "Requirement already satisfied: pytest<9.0.0,>=8.3.3 in /usr/local/lib/python3.11/dist-packages (from random-word==1.0.13->gptqmodel) (8.3.5)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from random-word==1.0.13->gptqmodel) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from random-word==1.0.13->gptqmodel) (2.32.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.3.0->gptqmodel) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gptqmodel) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->gptqmodel) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (0.21.1)\n",
            "Requirement already satisfied: pycodestyle>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from autopep8<3.0.0,>=2.3.1->random-word==1.0.13->gptqmodel) (2.13.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (3.11.15)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random-word==1.0.13->gptqmodel) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random-word==1.0.13->gptqmodel) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random-word==1.0.13->gptqmodel) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random-word==1.0.13->gptqmodel) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random-word==1.0.13->gptqmodel) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random-word==1.0.13->gptqmodel) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->gptqmodel) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.2.0->gptqmodel) (1.17.0)\n",
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
          ]
        }
      ],
      "source": [
        "!pip install gptqmodel\n",
        "from gptqmodel.models.auto import MODEL_MAP, SUPPORTED_MODELS\n",
        "from gptqmodel.models.base import BaseGPTQModel\n",
        "\n",
        "class LladaGPTQ(BaseGPTQModel):\n",
        "    base_modules = [\"model.transformer.wte\", \"model.transformer.ln_f\"]\n",
        "    pre_lm_head_norm_module = \"model.transformer.ln_f\"\n",
        "    lm_head = \"model.transformer.ff_out\"\n",
        "    layers_node = \"model.transformer.blocks\"       # ← repeating-layer container\n",
        "    layer_type  = \"LLaDALlamaBlock\"\n",
        "    layer_modules = [\n",
        "        [\"attn_out\", \"k_proj\", \"v_proj\", \"q_proj\"],\n",
        "        [\"ff_proj\", \"up_proj\"],\n",
        "        [\"ff_out\"],\n",
        "    ]\n",
        "\n",
        "# register for the auto-loader\n",
        "MODEL_MAP[\"llada\"] = LladaGPTQ\n",
        "if isinstance(SUPPORTED_MODELS, list):          # v2.x\n",
        "    SUPPORTED_MODELS.append(\"llada\")\n",
        "else:                                           # legacy dict style\n",
        "    SUPPORTED_MODELS[\"llada\"] = LladaGPTQ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c0b7a326de544379e6f272bb16950dd",
            "d156a752c0ed4c7aae3089454fffee46",
            "37ed86b9b5a14c769ac7b3355d5ed205",
            "06671c42d6cd417a8408497770845e28",
            "3fa0288cb5f74a358d08d378a760a82e",
            "9b941dc55a504769934ba5875ca6909a",
            "6b292f0134c14a83aaa12e71525b6c77",
            "2f95dd0eb0b64f40bd5ff8259e859d4e",
            "ec17703232514c698e494da39b10b8c1",
            "1206c416bd994d49894a2d6dc30b09ed",
            "e65e4a132d0b4f75a055bcbc439ea7b5"
          ]
        },
        "id": "QCZOcvJ8iPiH",
        "outputId": "79d9a327-d454-4d58-a43e-db2a31b5152b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "from_quantized: adapter: None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0b7a326de544379e6f272bb16950dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n",
            "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
            "\u001b[32mINFO\u001b[0m  The layer model.transformer.ff_out is not quantized.                     \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
            "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[ExllamaQuantLinear, TritonV2QuantLinear, TorchQuantLinear]`\n",
            "\u001b[32mINFO\u001b[0m  Kernel: selected -> `ExllamaQuantLinear`.                                \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.045867204666137695s                       \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
            "\u001b[33mWARN\u001b[0m  can't get model's sequence length from model config, will set to 4096.   \n",
            "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
            "  \"eos_token_id\": 126081,\n",
            "  \"pad_token_id\": 126081,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
            "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n",
            "  \"bos_token_id\": 126080,\n",
            "  \"eos_token_id\": 126081\n",
            "}\n",
            "\n",
            "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[ExllamaQuantLinear]`                                 \n",
            "\n",
            "Evaluating Single Question:\n",
            "Question: What is the value of \\( x \\) such that \\( 2x + 3 = 7 \\)? Give your final boxed answer.\n",
            "\n",
            "Running Backmasking...\n",
            "===== Generation Started (PRM Window Score + Batch Refine) =====\n",
            "Batch Size: 1\n",
            "Prompt (Item 0): user\n",
            "\n",
            "What is the value of \\( x \\) such that \\( 2x + 3 = 7 \\)? Give your final boxed answer.assistan...\n",
            "Config: Steps=128, GenLen=512, BlockLen=32, K=8, Thresh=0.8, N_Samples=5, SelectMetric=product\n",
            "\n",
            "--- Generating Block 1/16 ---\n",
            "\n",
            "--- Generating Block 2/16 ---\n",
            "\n",
            "--- Generating Block 3/16 ---\n",
            "\n",
            "--- Generating Block 4/16 ---\n",
            "\n",
            "--- Generating Block 5/16 ---\n",
            "\n",
            "--- Generating Block 6/16 ---\n",
            "\n",
            "--- Generating Block 7/16 ---\n",
            "\n",
            "--- Generating Block 8/16 ---\n",
            "\n",
            "--- Evaluating Window: Blocks 1 to 8 ---\n",
            "Scoring current state (before potential refinement)...\n",
            "\n",
            "[PRM ERROR] Apologies! The reward model could not be evaluated on this hardware ('bool' object has no attribute 'scatter'). Falling back to random block scores so generation can continue.\n",
            "\n",
            "Item 0: Min score 0.1519 < threshold (0.8000). Needs refinement.\n",
            "--- Starting Batch Refinement for 1 items ---\n",
            "Total tokens selected for backmasking across batch: 35\n",
            "Running batched refinement for 5 samples...\n",
            "Refinement using approx 128 steps per sample.\n",
            "Refinement limit mask set for tokens 41 to 297\n",
            "Error during model inference: The temp_state buffer is too small in the exllama backend for GPTQ with act-order. Please call the exllama_set_max_input_length function to increase the buffer size for a sequence length >=2765:\n",
            "from gptqmodel import exllama_set_max_input_length\n",
            "model = exllama_set_max_input_length(model, max_input_length=2765)\n",
            "Refined 1455 tokens over 128 steps for 5 samples.\n",
            "Scoring refined samples...\n",
            "\n",
            "[PRM ERROR] Apologies! The reward model could not be evaluated on this hardware ('bool' object has no attribute 'scatter'). Falling back to random block scores so generation can continue.\n",
            "\n",
            "Selecting best sample for each original batch item...\n",
            "Item 0: Chose sample 3 (Metric: 0.003359, Scores: ['0.757', '0.873', '0.609', '0.194', '0.297', '0.960', '0.240', '0.628'])\n",
            "\n",
            "--- Generating Block 9/16 ---\n",
            "\n",
            "--- Generating Block 10/16 ---\n",
            "\n",
            "--- Generating Block 11/16 ---\n",
            "\n",
            "--- Generating Block 12/16 ---\n",
            "\n",
            "--- Generating Block 13/16 ---\n",
            "\n",
            "--- Generating Block 14/16 ---\n",
            "\n",
            "--- Generating Block 15/16 ---\n",
            "\n",
            "--- Generating Block 16/16 ---\n",
            "\n",
            "--- Evaluating Window: Blocks 9 to 16 ---\n",
            "Scoring current state (before potential refinement)...\n",
            "\n",
            "[PRM ERROR] Apologies! The reward model could not be evaluated on this hardware ('bool' object has no attribute 'scatter'). Falling back to random block scores so generation can continue.\n",
            "\n",
            "Item 0: Min score 0.3251 < threshold (0.8000). Needs refinement.\n",
            "--- Starting Batch Refinement for 1 items ---\n",
            "Total tokens selected for backmasking across batch: 37\n",
            "Running batched refinement for 5 samples...\n",
            "Refinement using approx 72 steps per sample.\n",
            "Refinement limit mask set for tokens 297 to 553\n",
            "Error during model inference: The temp_state buffer is too small in the exllama backend for GPTQ with act-order. Please call the exllama_set_max_input_length function to increase the buffer size for a sequence length >=2765:\n",
            "from gptqmodel import exllama_set_max_input_length\n",
            "model = exllama_set_max_input_length(model, max_input_length=2765)\n",
            "Refined 360 tokens over 72 steps for 5 samples.\n",
            "Scoring refined samples...\n",
            "\n",
            "[PRM ERROR] Apologies! The reward model could not be evaluated on this hardware ('bool' object has no attribute 'scatter'). Falling back to random block scores so generation can continue.\n",
            "\n",
            "Selecting best sample for each original batch item...\n",
            "Item 0: Chose sample 0 (Metric: 0.001959, Scores: ['0.085', '0.506', '0.807', '0.706', '0.558', '0.536', '0.442', '0.602'])\n",
            "\n",
            "===== Generation Complete =====\n",
            "Warning (Item 0): 72 mask tokens remain.\n",
            "Final block scores (Item 0): ['0.757', '0.873', '0.609', '0.194', '0.297', '0.960', '0.240', '0.628', '0.085', '0.506', '0.807', '0.706', '0.558', '0.536', '0.442', '0.602']\n",
            "\n",
            "Generated output (Item 0, first 500 chars):\n",
            "To solve the equation \\( 2x + 3 = 7 \\), we will follow these steps:\n",
            "\n",
            "1.1: Start To isolating the term x We do by subtracting both 2x + 3 - 3 = 7 - 3\n",
            "   \\]\n",
            "   Simplifying both sides, we get:\n",
            "   \\[\n",
            "   2x = 4\n",
            " \\].2:, need to for \\( by dividing both sides of the equation by 2 of \\( x.\n",
            "   \\[\n",
            "   \\frac{2x}{2} = \\frac{4}{2}\n",
            "   \\]\n",
            "   Simplifying both sides, we get:\n",
            "   \\[\n",
            "   x = 2\n",
            "   \\]\n",
            "\n",
            "Therefore, the value of \\( x \\) is \\(\\boxed{2}\\).\n",
            "\n",
            "\n",
            "--- Backmasking Output ---\n",
            "To solve the equation \\( 2x + 3 = 7 \\), we will follow these steps:\n",
            "\n",
            "1.1: Start To isolating the term x We do by subtracting both 2x + 3 - 3 = 7 - 3\n",
            "   \\]\n",
            "   Simplifying both sides, we get:\n",
            "   \\[\n",
            "   2x = 4\n",
            " \\].2:, need to for \\( by dividing both sides of the equation by 2 of \\( x.\n",
            "   \\[\n",
            "   \\frac{2x}{2} = \\frac{4}{2}\n",
            "   \\]\n",
            "   Simplifying both sides, we get:\n",
            "   \\[\n",
            "   x = 2\n",
            "   \\]\n",
            "\n",
            "Therefore, the value of \\( x \\) is \\(\\boxed{2}\\).\n",
            "\n",
            "Backmasking Time: 136.41s\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from gptqmodel import  GPTQModel, QuantizeConfig,  BACKEND\n",
        "from gptqmodel.models.base import BaseGPTQModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "from gptqmodel.models.auto import MODEL_MAP\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#use this instead so the model runs, it will be replaced later\n",
        "PRM_MODEL_NAME = \"HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2\"\n",
        "\n",
        "\n",
        "\n",
        "#if you have more compute you can use Qwen/Qwen2.5-Math-PRM-7B\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    quantize_config = QuantizeConfig(\n",
        "        bits=8,\n",
        "        group_size=128,\n",
        "        desc_act = True,\n",
        "        sym=False\n",
        "    )\n",
        "    model = GPTQModel.load(\n",
        "        \"FunAGI/LLaDA-8B-Instruct-gptqmodel-4bit\",\n",
        "        device=DEVICE,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\")\n",
        "\n",
        "    prm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        PRM_MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        cache_dir=\"/my_vol\",\n",
        "        local_files_only=False,\n",
        "    )\n",
        "    prm_model = AutoModel.from_pretrained(\n",
        "        PRM_MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        cache_dir=\"/my_vol\",\n",
        "        local_files_only=False,\n",
        "    ).to(DEVICE).eval()\n",
        "\n",
        "    prm_model = prm_model.half().to(DEVICE).eval()\n",
        "\n",
        "    # 🧪 Your custom test question\n",
        "    question = \"What is the value of \\\\( x \\\\) such that \\\\( 2x + 3 = 7 \\\\)? Give your final boxed answer.\"\n",
        "    ground_truth = \"x = 2\"\n",
        "    run_single_prompt(question, model, tokenizer, prm_model, prm_tokenizer, ground_truth)\n",
        "\n",
        "    print(\"Script finished.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06671c42d6cd417a8408497770845e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1206c416bd994d49894a2d6dc30b09ed",
            "placeholder": "​",
            "style": "IPY_MODEL_e65e4a132d0b4f75a055bcbc439ea7b5",
            "value": " 16/16 [00:00&lt;00:00, 1663.87it/s]"
          }
        },
        "068cb134405d43c2830c6e0e87f55c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07501010fa4c4b6380a9fea237562844": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0c0b7a326de544379e6f272bb16950dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d156a752c0ed4c7aae3089454fffee46",
              "IPY_MODEL_37ed86b9b5a14c769ac7b3355d5ed205",
              "IPY_MODEL_06671c42d6cd417a8408497770845e28"
            ],
            "layout": "IPY_MODEL_3fa0288cb5f74a358d08d378a760a82e"
          }
        },
        "1206c416bd994d49894a2d6dc30b09ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f95dd0eb0b64f40bd5ff8259e859d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ed86b9b5a14c769ac7b3355d5ed205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f95dd0eb0b64f40bd5ff8259e859d4e",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec17703232514c698e494da39b10b8c1",
            "value": 16
          }
        },
        "3fa0288cb5f74a358d08d378a760a82e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ec0c7c456342b7b42de193640f14f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5c1b253d63594533b0a63d7bf0c21862": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_836f24ccdae549a2bed4e8169025bc22",
            "style": "IPY_MODEL_c534cf84f04a460680539df7da7c704e",
            "value": true
          }
        },
        "68eabe4bc4c54b41b51a4d37a039357e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7071a4f47d8949e8bf26ed9570502608",
            "placeholder": "​",
            "style": "IPY_MODEL_068cb134405d43c2830c6e0e87f55c31",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "6b292f0134c14a83aaa12e71525b6c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7071a4f47d8949e8bf26ed9570502608": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836f24ccdae549a2bed4e8169025bc22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad4f22e34c042d1afea297126e6dfe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b941dc55a504769934ba5875ca6909a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2bc0b92856e44de9c181de2cb290022": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4fe9e413cc042a1840f7876f5b03a49",
              "IPY_MODEL_a3693b4fd01048ba832f9f676d8296f7",
              "IPY_MODEL_5c1b253d63594533b0a63d7bf0c21862",
              "IPY_MODEL_f8f40c7bbb664c628b4982ae35e2656e",
              "IPY_MODEL_68eabe4bc4c54b41b51a4d37a039357e"
            ],
            "layout": "IPY_MODEL_07501010fa4c4b6380a9fea237562844"
          }
        },
        "a3693b4fd01048ba832f9f676d8296f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b8e4fecf8d224b8b9b3593124f8d6b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_f2f64b46a77a491d9a15d9d3ab60f0b2",
            "value": ""
          }
        },
        "a955f2cdd4d147938cbc979ef52b94df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8e4fecf8d224b8b9b3593124f8d6b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f2e5f1b9824be88d0938e37de712fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c534cf84f04a460680539df7da7c704e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d156a752c0ed4c7aae3089454fffee46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b941dc55a504769934ba5875ca6909a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b292f0134c14a83aaa12e71525b6c77",
            "value": "Fetching 16 files: 100%"
          }
        },
        "d4fe9e413cc042a1840f7876f5b03a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad4f22e34c042d1afea297126e6dfe9",
            "placeholder": "​",
            "style": "IPY_MODEL_a955f2cdd4d147938cbc979ef52b94df",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e65e4a132d0b4f75a055bcbc439ea7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec17703232514c698e494da39b10b8c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2f64b46a77a491d9a15d9d3ab60f0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8f40c7bbb664c628b4982ae35e2656e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b9f2e5f1b9824be88d0938e37de712fa",
            "style": "IPY_MODEL_51ec0c7c456342b7b42de193640f14f5",
            "tooltip": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
