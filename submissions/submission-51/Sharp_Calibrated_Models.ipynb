{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Hyo5xgDhpkIW",
        "ZKKlYsQpTSMl",
        "5BuDhNeUTVYg",
        "ogPSZzH5Tt4m",
        "8SHaoDtLUTKQ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import traceback\n",
        "import os\n",
        "import contextlib\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from typing import Union\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "x9IY0YbTSO3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the UCI datasets"
      ],
      "metadata": {
        "id": "Hyo5xgDhpkIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git remote add origin https://github.com/YoungseogChung/calibrated-quantile-uq.git\n",
        "!git config core.sparseCheckout true\n",
        "!echo \"data/UCI_Datasets\" >> .git/info/sparse-checkout\n",
        "!git pull origin master\n",
        "!mkdir -p datasets/UCI_Datasets\n",
        "!mv data/UCI_Datasets/* datasets/UCI_Datasets/\n",
        "!rm -rf data .git"
      ],
      "metadata": {
        "id": "88kc4eoeYbIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb623e3-6144-4e24-88a4-201532d35505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 228, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 228 (delta 119), reused 172 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (228/228), 3.68 MiB | 8.78 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "From https://github.com/YoungseogChung/calibrated-quantile-uq\n",
            " * branch            master     -> FETCH_HEAD\n",
            " * [new branch]      master     -> origin/master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1D interpolation function\n",
        "\n",
        "This function is used to interpolate additional quantiles between the quantile regression models."
      ],
      "metadata": {
        "id": "ZKKlYsQpTSMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Interp1d(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y, xnew, out=None):\n",
        "        \"\"\"\n",
        "        Linear 1D interpolation on the GPU for Pytorch.\n",
        "        This function returns interpolated values of a set of 1-D functions at\n",
        "        the desired query points `xnew`.\n",
        "        This function is working similarly to Matlab™ or scipy functions with\n",
        "        the `linear` interpolation mode on, except that it parallelises over\n",
        "        any number of desired interpolation problems.\n",
        "        The code will run on GPU if all the tensors provided are on a cuda\n",
        "        device.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : (N, ) or (D, N) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values.\n",
        "        y : (N,) or (D, N) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values. The length of `y` along its\n",
        "            last dimension must be the same as that of `x`\n",
        "        xnew : (P,) or (D, P) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values. `xnew` can only be 1-D if\n",
        "            _both_ `x` and `y` are 1-D. Otherwise, its length along the first\n",
        "            dimension must be the same as that of whichever `x` and `y` is 2-D.\n",
        "        out : Pytorch Tensor, same shape as `xnew`\n",
        "            Tensor for the output. If None: allocated automatically.\n",
        "\n",
        "        \"\"\"\n",
        "        # making the vectors at least 2D\n",
        "        is_flat = {}\n",
        "        require_grad = {}\n",
        "        v = {}\n",
        "        device = []\n",
        "        eps = torch.finfo(y.dtype).eps\n",
        "        for name, vec in {'x': x, 'y': y, 'xnew': xnew}.items():\n",
        "            assert len(vec.shape) <= 2, 'interp1d: all inputs must be '\\\n",
        "                                        'at most 2-D.'\n",
        "            if len(vec.shape) == 1:\n",
        "                v[name] = vec[None, :]\n",
        "            else:\n",
        "                v[name] = vec\n",
        "            is_flat[name] = v[name].shape[0] == 1\n",
        "            require_grad[name] = vec.requires_grad\n",
        "            device = list(set(device + [str(vec.device)]))\n",
        "        assert len(device) == 1, 'All parameters must be on the same device.'\n",
        "        device = device[0]\n",
        "\n",
        "        # Checking for the dimensions\n",
        "        assert (v['x'].shape[1] == v['y'].shape[1]\n",
        "                and (\n",
        "                     v['x'].shape[0] == v['y'].shape[0]\n",
        "                     or v['x'].shape[0] == 1\n",
        "                     or v['y'].shape[0] == 1\n",
        "                    )\n",
        "                ), (\"x and y must have the same number of columns, and either \"\n",
        "                    \"the same number of row or one of them having only one \"\n",
        "                    \"row.\")\n",
        "\n",
        "        reshaped_xnew = False\n",
        "        if ((v['x'].shape[0] == 1) and (v['y'].shape[0] == 1)\n",
        "           and (v['xnew'].shape[0] > 1)):\n",
        "            # if there is only one row for both x and y, there is no need to\n",
        "            # loop over the rows of xnew because they will all have to face the\n",
        "            # same interpolation problem. We should just stack them together to\n",
        "            # call interp1d and put them back in place afterwards.\n",
        "            original_xnew_shape = v['xnew'].shape\n",
        "            v['xnew'] = v['xnew'].contiguous().view(1, -1)\n",
        "            reshaped_xnew = True\n",
        "\n",
        "        # identify the dimensions of output and check if the one provided is ok\n",
        "        D = max(v['x'].shape[0], v['xnew'].shape[0])\n",
        "        shape_ynew = (D, v['xnew'].shape[-1])\n",
        "        if out is not None:\n",
        "            if out.numel() != shape_ynew[0]*shape_ynew[1]:\n",
        "                # The output provided is of incorrect shape.\n",
        "                # Going for a new one\n",
        "                out = None\n",
        "            else:\n",
        "                ynew = out.reshape(shape_ynew)\n",
        "        if out is None:\n",
        "            ynew = torch.zeros(*shape_ynew, device=device)\n",
        "\n",
        "        # moving everything to the desired device in case it was not there\n",
        "        # already (not handling the case things do not fit entirely, user will\n",
        "        # do it if required.)\n",
        "        for name in v:\n",
        "            v[name] = v[name].to(device)\n",
        "\n",
        "        # calling searchsorted on the x values.\n",
        "        ind = ynew.long()\n",
        "\n",
        "        # expanding xnew to match the number of rows of x in case only one xnew is\n",
        "        # provided\n",
        "        if v['xnew'].shape[0] == 1:\n",
        "            v['xnew'] = v['xnew'].expand(v['x'].shape[0], -1)\n",
        "\n",
        "        # the squeeze is because torch.searchsorted does accept either a nd with\n",
        "        # matching shapes for x and xnew or a 1d vector for x. Here we would\n",
        "        # have (1,len) for x sometimes\n",
        "        torch.searchsorted(v['x'].contiguous().squeeze(),\n",
        "                           v['xnew'].contiguous(), out=ind)\n",
        "\n",
        "        # the `-1` is because searchsorted looks for the index where the values\n",
        "        # must be inserted to preserve order. And we want the index of the\n",
        "        # preceeding value.\n",
        "        ind -= 1\n",
        "        # we clamp the index, because the number of intervals is x.shape-1,\n",
        "        # and the left neighbour should hence be at most number of intervals\n",
        "        # -1, i.e. number of columns in x -2\n",
        "        ind = torch.clamp(ind, 0, v['x'].shape[1] - 1 - 1)\n",
        "\n",
        "        # helper function to select stuff according to the found indices.\n",
        "        def sel(name):\n",
        "            if is_flat[name]:\n",
        "                return v[name].contiguous().view(-1)[ind]\n",
        "            return torch.gather(v[name], 1, ind)\n",
        "\n",
        "        # activating gradient storing for everything now\n",
        "        enable_grad = False\n",
        "        saved_inputs = []\n",
        "        for name in ['x', 'y', 'xnew']:\n",
        "            if require_grad[name]:\n",
        "                enable_grad = True\n",
        "                saved_inputs += [v[name]]\n",
        "            else:\n",
        "                saved_inputs += [None, ]\n",
        "        # assuming x are sorted in the dimension 1, computing the slopes for\n",
        "        # the segments\n",
        "        is_flat['slopes'] = is_flat['x']\n",
        "        # now we have found the indices of the neighbors, we start building the\n",
        "        # output. Hence, we start also activating gradient tracking\n",
        "        with torch.enable_grad() if enable_grad else contextlib.suppress():\n",
        "            v['slopes'] = (\n",
        "                    (v['y'][:, 1:]-v['y'][:, :-1])\n",
        "                    /\n",
        "                    (eps + (v['x'][:, 1:]-v['x'][:, :-1]))\n",
        "                )\n",
        "\n",
        "            # now build the linear interpolation\n",
        "            ynew = sel('y') + sel('slopes')*(\n",
        "                                    v['xnew'] - sel('x'))\n",
        "\n",
        "            if reshaped_xnew:\n",
        "                ynew = ynew.view(original_xnew_shape)\n",
        "\n",
        "        ctx.save_for_backward(ynew, *saved_inputs)\n",
        "        return ynew\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        inputs = ctx.saved_tensors[1:]\n",
        "        gradients = torch.autograd.grad(\n",
        "                        ctx.saved_tensors[0],\n",
        "                        [i for i in inputs if i is not None],\n",
        "                        grad_out, retain_graph=True)\n",
        "        result = [None, ] * 5\n",
        "        pos = 0\n",
        "        for index in range(len(inputs)):\n",
        "            if inputs[index] is not None:\n",
        "                result[index] = gradients[pos]\n",
        "                pos += 1\n",
        "        return (*result,)\n",
        "\n",
        "interp1d = Interp1d.apply"
      ],
      "metadata": {
        "id": "_hVKbPy3S09B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CaliPSo model ensemble definition\n",
        "\n",
        "Implementation of the CaliPSo model as described in the paper"
      ],
      "metadata": {
        "id": "5BuDhNeUTVYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LOWER QUANTILE MODEL\n",
        "class quantile_model(nn.Module):\n",
        "    def __init__(self, X, Y, vanilla_model, quantile):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.quantile = quantile\n",
        "        self.vanilla_model = vanilla_model\n",
        "\n",
        "    def forward(self, x, x_cal = None, y_cal = None):\n",
        "\n",
        "        vanilla_model = self.vanilla_model(x)\n",
        "\n",
        "        if x_cal is None and y_cal is None:\n",
        "            y_cal = self.Y\n",
        "            vanilla_model_Xcal = self.vanilla_model(self.X)\n",
        "        else:\n",
        "            vanilla_model_Xcal = self.vanilla_model(x_cal)\n",
        "\n",
        "        #c_lb enforces calibration during training on calibration dataset\n",
        "        c_lb = torch.quantile(y_cal - vanilla_model_Xcal, self.quantile, interpolation='linear')\n",
        "\n",
        "        zero_quantile = c_lb + vanilla_model\n",
        "\n",
        "        return zero_quantile\n",
        "\n",
        "## Beyond Pinball Loss Base Model\n",
        "class bpl_nn(nn.Module):\n",
        "    def __init__(self, nfeatures):\n",
        "        super(bpl_nn, self).__init__()\n",
        "        # input layer\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(nfeatures, 64),\n",
        "            nn.ReLU())\n",
        "        # hidden layers\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(1):\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()))\n",
        "        # output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.input_layer(input)\n",
        "        for layer in self.hidden_layers:\n",
        "            output_hidden = layer(output) + output\n",
        "            output = output_hidden\n",
        "        output = self.output_layer(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "## SPECIFY CALIBRATED QUANTILE MODEL ENSEMBLE\n",
        "class quantile_model_ensemble(nn.Module):\n",
        "    def __init__(self, X, Y, vanilla_model, half_q_levels: Union[int, torch.Tensor], output_device, vanilla_weights_path = None, avg_weights=False):\n",
        "        super().__init__()\n",
        "        self.output_device = output_device\n",
        "        self.upper_quantile_models = nn.ModuleList()\n",
        "        self.lower_quantile_models = nn.ModuleList()\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.iso_reg = None\n",
        "        self.printname = 'quantile_model'\n",
        "        self.printcolor = 'C0'\n",
        "\n",
        "        ## check if number of quantile levels is divisible by two\n",
        "        if isinstance(half_q_levels, int):\n",
        "            assert int((half_q_levels+1)/2) == (half_q_levels+1)/2\n",
        "            quantile_model_levels = range(int((half_q_levels+1)/2))\n",
        "        else:\n",
        "            quantile_model_levels = half_q_levels\n",
        "\n",
        "        self.half_q_levels = quantile_model_levels\n",
        "\n",
        "        length_data = X.shape[0]\n",
        "\n",
        "        for i in range(len(quantile_model_levels)):\n",
        "            vanilla_quantile_model_lower = vanilla_model(nfeatures = X.shape[1])\n",
        "            vanilla_quantile_model_upper = vanilla_model(nfeatures = X.shape[1])\n",
        "            if isinstance(vanilla_weights_path, list):\n",
        "                path = vanilla_weights_path[i]\n",
        "            else:\n",
        "                path = vanilla_weights_path\n",
        "            if path is not None:\n",
        "                # Initialize weights from normal regression on training set\n",
        "                lower_weights = torch.load(path, weights_only=True, map_location=output_device)\n",
        "                upper_weights = copy.deepcopy(lower_weights)\n",
        "                if avg_weights:\n",
        "                    for key in lower_weights:\n",
        "                        upper_weights[key] = (lower_weights[key].to(output_device) + vanilla_quantile_model_upper.state_dict()[key].to(output_device)) / 2\n",
        "                        lower_weights[key] = (lower_weights[key].to(output_device) + vanilla_quantile_model_lower.state_dict()[key].to(output_device)) / 2\n",
        "                vanilla_quantile_model_lower.load_state_dict(lower_weights)\n",
        "                vanilla_quantile_model_upper.load_state_dict(upper_weights)\n",
        "            q_model_upper = quantile_model(X=X, Y=Y, vanilla_model=vanilla_quantile_model_upper, quantile = 1)\n",
        "            q_model_lower = quantile_model(X=X, Y=Y, vanilla_model=vanilla_quantile_model_lower, quantile = 0)\n",
        "            self.upper_quantile_models.append(q_model_upper)\n",
        "            self.lower_quantile_models.append(q_model_lower)\n",
        "\n",
        "        self.to(output_device)\n",
        "\n",
        "\n",
        "    def forward(self, x, quantile = []):\n",
        "\n",
        "        half_q_levels = self.half_q_levels\n",
        "        output_device = self.output_device\n",
        "        n_quantile_models = len(half_q_levels)\n",
        "\n",
        "\n",
        "        quantiles_lower = []\n",
        "        quantiles_upper = []\n",
        "\n",
        "        x_cal = self.X\n",
        "        y_cal = self.Y\n",
        "\n",
        "        interval_tot = 1\n",
        "        ind_kept = torch.ones_like(y_cal, dtype=torch.bool)\n",
        "\n",
        "        for n_mod in range(n_quantile_models):\n",
        "            upper_quantile_model = self.upper_quantile_models[n_mod]\n",
        "            lower_quantile_model = self.lower_quantile_models[n_mod]\n",
        "\n",
        "            lower_quantile = lower_quantile_model(x, x_cal = x_cal, y_cal = y_cal)\n",
        "            upper_quantile = upper_quantile_model(x, x_cal = x_cal, y_cal = y_cal)\n",
        "\n",
        "            # USED EXCLUSIVELY TO COMPUTE CALIBRATING ELEMENTS\n",
        "            lower_quantile_cal = lower_quantile_model(x_cal, x_cal=x_cal, y_cal=y_cal)\n",
        "            upper_quantile_cal = upper_quantile_model(x_cal, x_cal=x_cal, y_cal=y_cal)\n",
        "\n",
        "\n",
        "            # IN TRAINING, WE ONLY OPTIMIZE SHARPNESS FOR ENTRIES CORRESPONDING TO x_cal AND y_cal\n",
        "            if self.training:\n",
        "                # ALL ENTRIES THAT DO NOT CORRESPOND TO x_cal AND y_cal ARE REPLACED WITH DUMMY VALUES THAT DO NOT\n",
        "                # AFFECT OPTIMIZATION\n",
        "                # index_x = np.all([np.in1d(x.T[i].cpu(), x_cal.T[i].cpu()) for i in range(x.shape[1])], 0)\n",
        "                index_x = np.all([np.isin(x.T[i].cpu(), x_cal.T[i].cpu()) for i in range(x.shape[1])], 0) #in1d\n",
        "                lower_quantile[index_x.__invert__()] = 0\n",
        "                upper_quantile[index_x.__invert__()] = 0\n",
        "            else:\n",
        "                if n_mod==0:\n",
        "                    one_quantile = upper_quantile\n",
        "                    one_quantile_cal = upper_quantile_cal\n",
        "                else:\n",
        "                    # Apply maximum and minimum operations to inner models as described in the paper\n",
        "                    lower_quantile = torch.maximum(lower_quantile, previous_lower_quantile + delta_lower * (\n",
        "                                previous_upper_quantile - previous_lower_quantile)).to(self.output_device)\n",
        "                    upper_quantile = torch.minimum(upper_quantile, previous_lower_quantile + delta_upper * (\n",
        "                                previous_upper_quantile - previous_lower_quantile)).to(self.output_device)\n",
        "                    lower_quantile = torch.minimum(lower_quantile, one_quantile).to(self.output_device)\n",
        "\n",
        "                    lower_quantile_cal = torch.maximum(lower_quantile_cal, previous_lower_quantile_cal[ind_kept] + delta_lower * (\n",
        "                                previous_upper_quantile_cal[ind_kept] - previous_lower_quantile_cal[ind_kept])).to(self.output_device)\n",
        "                    upper_quantile_cal = torch.minimum(upper_quantile_cal, previous_lower_quantile_cal[ind_kept] + delta_upper * (\n",
        "                                previous_upper_quantile_cal[ind_kept] - previous_lower_quantile_cal[ind_kept])).to(self.output_device)\n",
        "                    lower_quantile_cal = torch.minimum(lower_quantile_cal, one_quantile_cal[ind_kept]).to(self.output_device)\n",
        "\n",
        "                upper_quantile = torch.maximum(lower_quantile, upper_quantile).to(self.output_device)\n",
        "                previous_lower_quantile = lower_quantile\n",
        "                previous_upper_quantile = upper_quantile\n",
        "                previous_lower_quantile_cal = lower_quantile_cal\n",
        "                previous_upper_quantile_cal = upper_quantile_cal\n",
        "                one_quantile_cal = one_quantile_cal[ind_kept.squeeze()]\n",
        "\n",
        "            quantiles_lower.append(lower_quantile)\n",
        "            quantiles_upper.append(upper_quantile)\n",
        "\n",
        "            if n_mod<n_quantile_models-1:\n",
        "                delta_upper_lower = upper_quantile_cal - lower_quantile_cal\n",
        "                delta_upper_lower = torch.maximum(delta_upper_lower, torch.tensor(1e-24))\n",
        "                delta_quantile_level = (half_q_levels[n_mod+1] - half_q_levels[n_mod]).to(self.output_device)\n",
        "                delta_upper = torch.quantile((y_cal - lower_quantile_cal)/delta_upper_lower, 1 - delta_quantile_level/interval_tot)\n",
        "                delta_lower = torch.quantile((y_cal - lower_quantile_cal) / delta_upper_lower, delta_quantile_level/interval_tot)\n",
        "                ind_kept = ((y_cal < lower_quantile_cal + delta_upper*(delta_upper_lower)) & (\n",
        "                        y_cal > lower_quantile_cal + delta_lower*(delta_upper_lower))).reshape(y_cal.shape[0], )\n",
        "                if not ind_kept.any() and n_mod < n_quantile_models-1:\n",
        "                    STOP_FOR_DEBUGGING = 1\n",
        "                x_cal = x_cal[ind_kept]\n",
        "                y_cal = y_cal[ind_kept]\n",
        "\n",
        "\n",
        "                interval_tot = 1 - 2*half_q_levels[n_mod+1]\n",
        "\n",
        "        quantiles_upper.reverse()\n",
        "        quantiles = torch.hstack([torch.hstack(quantiles_lower).to(output_device), torch.hstack(quantiles_upper).to(output_device)]).to(output_device)\n",
        "        if quantile == []:\n",
        "            return quantiles\n",
        "        else:\n",
        "            if not torch.is_tensor(quantile):\n",
        "                quantile = torch.tensor(quantile)\n",
        "            quantile = quantile.to(self.output_device)\n",
        "            total_q_levels = torch.hstack([half_q_levels, 1- half_q_levels.flip(0) ]).to(self.output_device)\n",
        "            return interp1d(total_q_levels.repeat(x.shape[0],1), quantiles, quantile)\n",
        "\n",
        "    def get_quantiles(self, X, q_levels):\n",
        "        if self.iso_reg is None:\n",
        "            return self(X, q_levels)\n",
        "        else:\n",
        "            length_data = self.X.shape[0]\n",
        "            quantile_linspace = torch.linspace(1/(length_data+1), length_data/(length_data+1), length_data)\n",
        "            quantile_linspace = torch.linspace(0, 1, length_data)\n",
        "            uncalibrated_quantiles = self(X, quantile_linspace.to(self.output_device))\n",
        "            recalibrated_quantile_vals = self.iso_reg.predict(quantile_linspace.numpy())\n",
        "            recalibrated_quantile_vals = torch.tensor(recalibrated_quantile_vals).to(self.output_device)\n",
        "            if not torch.is_tensor(q_levels):\n",
        "                q_levels = torch.tensor(q_levels)\n",
        "            return interp1d(recalibrated_quantile_vals.repeat(X.shape[0], 1), uncalibrated_quantiles,\n",
        "                                            q_levels.to(self.output_device)).to(self.output_device)\n",
        "\n",
        "\n",
        "    def recalibrate(self, val=None):\n",
        "\n",
        "        self.iso_reg = None\n",
        "        self.eval()\n",
        "        if val is None:\n",
        "            cdf_vals = self.get_cdf_value(self.X, self.Y)\n",
        "        else:\n",
        "            cdf_vals = self.get_cdf_value(torch.cat((self.X, val[0]), dim=0), torch.cat((self.Y, val[1]), dim=0))\n",
        "        Phat_vals = []\n",
        "        for cdf in cdf_vals:\n",
        "            cdf_l = cdf_vals <= cdf\n",
        "            cdf_ind = torch.zeros(cdf_vals.shape)\n",
        "            cdf_ind[cdf_l] = 1\n",
        "            Phat_vals.append(torch.mean(cdf_ind))\n",
        "\n",
        "        Phat_vals = torch.stack(Phat_vals).reshape(cdf_vals.shape[0], 1)\n",
        "\n",
        "        ## CONVERT Phat_Vals and cdf_vals TO NUMPY FOR ISOTONIC REGRESSION\n",
        "        cdf_vals_np = np.float64(cdf_vals.detach().cpu().numpy())\n",
        "        Phat_vals_np = np.float64(Phat_vals.cpu().numpy().reshape(Phat_vals.shape[0], ))\n",
        "\n",
        "        # Recalibrate using Isotonic Regression\n",
        "        iso_reg = IsotonicRegression(out_of_bounds='clip').fit(cdf_vals_np, Phat_vals_np)\n",
        "        self.iso_reg = iso_reg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_cdf_value(self, X, Y, num_samples = 10000):\n",
        "        return self.get_cdf1_cdf2(X=X, Y1=Y, Y2=None, num_samples=num_samples)\n",
        "\n",
        "\n",
        "    def get_cdf1_cdf2(self, X, Y1, Y2 = None, num_samples = 10000):\n",
        "        # COMPUTE CDF FOR SAME VALUE OF X AND TWO DIFFERENT VALUES OF Y. REQUIRED TO COMPUTE\n",
        "        # FINITE DIFFERENCE USED TO COMPUTE LIKELIHOOD. SIMULTANEOUS COMPUTATION FOR DIFFERENT YS\n",
        "        # NECESSARY DUE TO SAMPLING-BASED COMPUTATION\n",
        "\n",
        "\n",
        "        output_device = self.output_device\n",
        "        length_data = X.shape[0]\n",
        "        q_levels = torch.linspace(0, 1, num_samples).to(output_device)\n",
        "\n",
        "        quantiles = self(X, quantile=q_levels)\n",
        "\n",
        "        F1 = interp1d(quantiles, q_levels, Y1).clamp(min=0, max=1)\n",
        "        if Y2 is None:\n",
        "            if self.iso_reg is None:\n",
        "                return F1\n",
        "            else:\n",
        "                F1_recal = self.iso_reg.predict(F1.detach().cpu().numpy())\n",
        "                return torch.tensor(F1_recal).to(output_device)\n",
        "        else:\n",
        "            F2 = interp1d(quantiles, q_levels, Y2).clamp(min=0, max=1)\n",
        "            if self.iso_reg is None:\n",
        "                return F1, F2\n",
        "            else:\n",
        "                F1_recal = self.iso_reg.predict(F1.detach().cpu().numpy())\n",
        "                F2_recal = self.iso_reg.predict(F2.detach().cpu().numpy())\n",
        "                return F1, F2\n",
        "\n",
        "\n",
        "    def get_likelihood(self, X, Y, deltay = 1e-2, num_samples = 10000):\n",
        "        output_device = self.output_device\n",
        "        dY = deltay*torch.ones(Y.shape).to(output_device)\n",
        "        Y_dplus = Y+dY\n",
        "        Y_dminus = Y-dY\n",
        "        F_minus, F_plus = self.get_cdf1_cdf2(X=X, Y1=Y_dminus, Y2=Y_dplus, num_samples = 10000)\n",
        "        likelihood = (F_plus - F_minus)/(2*dY.reshape(F_minus.shape))\n",
        "        return likelihood"
      ],
      "metadata": {
        "id": "bmxB5hpGSh-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute metrics for model evaluation\n",
        "\n",
        "Metrics include the Expected Calibration Error (ECE) and 95% Interval Sharpness"
      ],
      "metadata": {
        "id": "ogPSZzH5Tt4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_score(X_test, Y_test, model, p_diagnostics =torch.linspace(0, 1, 25)):\n",
        "\n",
        "        output_device = model.output_device\n",
        "        CS = torch.zeros(p_diagnostics.shape).to(output_device)\n",
        "        quantile_tau = model.get_quantiles(X_test, p_diagnostics)\n",
        "        diff = Y_test - quantile_tau\n",
        "        for i in range(p_diagnostics.shape[0]):\n",
        "            CS[i] = torch.mean(torch.maximum(p_diagnostics[i] * (diff.T[i]), (1 - p_diagnostics[i]) * (-diff.T[i]))).to(output_device)\n",
        "        return CS.mean()\n",
        "\n",
        "def expected_calibration_error(X_test, Y_test, model, p_diagnostics = torch.linspace(0, 1, 100)):\n",
        "\n",
        "    output_device = model.output_device\n",
        "    p_diagnostics = p_diagnostics.to(model.output_device)\n",
        "    cdf_vals = model.get_cdf_value(X=X_test, Y=Y_test)\n",
        "    phat = []\n",
        "    for pj in p_diagnostics:\n",
        "        cdf_l = cdf_vals <= pj\n",
        "        cdf_ind = torch.zeros(cdf_vals.shape).to(output_device)\n",
        "        cdf_ind[cdf_l] = 1\n",
        "        phat.append(torch.mean(cdf_ind))\n",
        "    phat = torch.stack(phat).reshape(p_diagnostics.shape[0],)\n",
        "    dp = p_diagnostics[1:]- p_diagnostics[:-1]\n",
        "    err_p = 0.5*(phat - p_diagnostics)[1:] + 0.5*(phat - p_diagnostics)[:-1]\n",
        "    return torch.sum(err_p**2*dp) # *dp\n",
        "\n",
        "\n",
        "def negative_log_likelihood(X_test, Y_test, model, dy = 1e-2):\n",
        "\n",
        "    output_device = model.output_device\n",
        "    dFdY = model.get_likelihood(X=X_test, Y=Y_test)\n",
        "\n",
        "    return -torch.mean(torch.log(dFdY).to(output_device)).to(output_device)\n",
        "\n",
        "\n",
        "\n",
        "def average_variance(X_test, model, p_diagnostics = torch.linspace(0, 1, 100)):\n",
        "\n",
        "    output_device = model.output_device\n",
        "    quantiles = model.get_quantiles(X=X_test, q_levels=p_diagnostics)\n",
        "    dp = (p_diagnostics[1:] - p_diagnostics[:-1]).to(output_device)\n",
        "    x = 0.5*(quantiles.T[1:] + quantiles.T[:-1])\n",
        "    Ex = torch.sum(x.T*dp,1)\n",
        "    varx = torch.sum(((x - Ex)**2).T*dp,1).to(output_device)\n",
        "\n",
        "    return torch.mean(varx)\n",
        "\n",
        "def average_95_interval(X_test, model):\n",
        "\n",
        "    output_device = model.output_device\n",
        "    quantiles = model.get_quantiles(X=X_test, q_levels=torch.tensor([0.025, 0.975]).to(output_device))\n",
        "    return torch.mean(quantiles.T[-1] - quantiles.T[0], 0).to(output_device)\n",
        "\n",
        "def run_diagnostics(X_test, Y_test, model, rnd=4, display=True):\n",
        "\n",
        "    # nll = round(negative_log_likelihood(X_test, Y_test, model).item(), rnd)\n",
        "    # print('Negative log likelihood: ', nll)\n",
        "    ece = round(expected_calibration_error(X_test, Y_test, model).item(), rnd)\n",
        "\n",
        "    avg_interval = round(average_95_interval(X_test, model).item(), rnd)\n",
        "    avg_var = round(average_variance(X_test, model).item(), rnd)\n",
        "    chk_score = round(check_score(X_test, Y_test, model).item(), rnd)\n",
        "    if display:\n",
        "        print('Expected calibration error: ', ece)\n",
        "        print('Average length of 95% interval: ', avg_interval)\n",
        "        print('Average variance: ', avg_var)\n",
        "        print('Check score: ', chk_score)\n",
        "\n",
        "    return np.array([ece, avg_interval, avg_var, chk_score])\n",
        "\n",
        "def plot_calibration_curve(\n",
        "    exp_proportions,\n",
        "    obs_proportions,\n",
        "    title=None,\n",
        "    curve_label=None,\n",
        "    make_plots=False,\n",
        "):\n",
        "    miscalibration_area = torch.mean(\n",
        "        torch.abs(exp_proportions - obs_proportions)\n",
        "    ).item()\n",
        "\n",
        "    return miscalibration_area\n",
        "\n",
        "def get_q_idx(exp_props, q):\n",
        "    target_idx = None\n",
        "    for idx, x in enumerate(exp_props):\n",
        "        if idx + 1 == exp_props.shape[0]:\n",
        "            if round(q, 2) == round(float(exp_props[-1]), 2):\n",
        "                target_idx = exp_props.shape[0] - 1\n",
        "            break\n",
        "        if x <= q < exp_props[idx + 1]:\n",
        "            target_idx = idx\n",
        "            break\n",
        "    if target_idx is None:\n",
        "        import pdb\n",
        "\n",
        "        pdb.set_trace()\n",
        "        raise ValueError(\"q must be within exp_props\")\n",
        "    return target_idx\n",
        "\n",
        "def test_group_cali(\n",
        "    y,\n",
        "    q_pred_mat,\n",
        "    exp_props,\n",
        "    y_range,\n",
        "    ratio,\n",
        "    num_group_draws=20,\n",
        "    make_plots=False,\n",
        "):\n",
        "\n",
        "    num_pts, num_q = q_pred_mat.shape\n",
        "    group_size = max([int(round(num_pts * ratio)), 2])\n",
        "    q_025_idx = get_q_idx(exp_props, 0.025)\n",
        "    q_975_idx = get_q_idx(exp_props, 0.975)\n",
        "\n",
        "    score_per_trial = []\n",
        "    for _ in range(20):\n",
        "        ##########\n",
        "        group_cali_scores = []\n",
        "        for g_idx in range(num_group_draws):\n",
        "            rand_idx = np.random.choice(num_pts, group_size, replace=True)\n",
        "            g_y = y[rand_idx]\n",
        "            g_q_preds = q_pred_mat[rand_idx, :]\n",
        "            g_obs_props = torch.mean(\n",
        "                (g_q_preds >= g_y).float(), dim=0\n",
        "            ).flatten()\n",
        "            assert exp_props.shape == g_obs_props.shape\n",
        "            g_cali_score = plot_calibration_curve(\n",
        "                exp_props, g_obs_props, make_plots=False\n",
        "            )\n",
        "\n",
        "            group_cali_scores.append(g_cali_score)\n",
        "\n",
        "        # mean_cali_score = np.mean(group_cali_scores)\n",
        "        mean_cali_score = np.max(group_cali_scores)\n",
        "        ##########\n",
        "\n",
        "        score_per_trial.append(mean_cali_score)\n",
        "\n",
        "    return np.mean(score_per_trial)\n",
        "    return mean_cali_score\n",
        "\n",
        "def test_uq(\n",
        "    model,\n",
        "    x,\n",
        "    y,\n",
        "    exp_props,\n",
        "    y_range,\n",
        "    recal_model=None,\n",
        "    recal_type=None,\n",
        "    make_plots=False,\n",
        "    test_group_cal=False,\n",
        "    display=True,\n",
        "):\n",
        "\n",
        "    num_pts = x.shape[0]\n",
        "    y = y.reshape(num_pts, -1)\n",
        "\n",
        "    quantile_preds = model.get_quantiles(\n",
        "        x,\n",
        "        exp_props,\n",
        "    )  # of shape (num_pts, num_q)\n",
        "    obs_props = torch.mean((quantile_preds >= y).float(), dim=0).flatten()\n",
        "\n",
        "    assert exp_props.shape == obs_props.shape\n",
        "\n",
        "    pred = model(x)\n",
        "    num_quantiles = pred.shape[1]\n",
        "    pinball_loss = []\n",
        "    model_obs = []\n",
        "    model_y = []\n",
        "    mask = (y >= pred)\n",
        "    delta = (y - pred)\n",
        "    for i in range(num_quantiles):\n",
        "        quantile = i/(num_quantiles-1)\n",
        "        q_mask = mask[:,i]\n",
        "        q_delta = delta[:,i]\n",
        "        pinball_loss.append(torch.mean(q_mask*q_delta*quantile + (~q_mask)*(-q_delta)*(1-quantile)).item())\n",
        "        model_obs.append(torch.mean((y <= pred[:,i].reshape(y.shape)).float()).item())\n",
        "        model_y.append(torch.mean(pred[:,i]).item())\n",
        "\n",
        "    idx_01 = get_q_idx(exp_props, 0.01)\n",
        "    idx_99 = get_q_idx(exp_props, 0.99)\n",
        "    individual_cali = exp_props[idx_01 : idx_99 + 1] - obs_props[idx_01 : idx_99 + 1]\n",
        "    cali_score = torch.abs(individual_cali).mean().item()\n",
        "\n",
        "    order = torch.argsort(y.flatten())\n",
        "    q_025 = quantile_preds[:, get_q_idx(exp_props, 0.025)][order]\n",
        "    q_975 = quantile_preds[:, get_q_idx(exp_props, 0.975)][order]\n",
        "    sharp_score = torch.mean(q_975 - q_025).item() / y_range\n",
        "\n",
        "    g_cali_scores = []\n",
        "    if test_group_cal:\n",
        "        ratio_arr = np.linspace(0.01, 1.0, 10)\n",
        "        if display:\n",
        "            print(\n",
        "                \"Spanning group size from {} to {} in {} increments\".format(\n",
        "                    np.min(ratio_arr), np.max(ratio_arr), len(ratio_arr)\n",
        "                )\n",
        "            )\n",
        "        for r in (tqdm.tqdm(ratio_arr) if display else ratio_arr):\n",
        "            gc = test_group_cali(\n",
        "                y=y,\n",
        "                q_pred_mat=quantile_preds[:, idx_01 : idx_99 + 1],\n",
        "                exp_props=exp_props[idx_01 : idx_99 + 1],\n",
        "                y_range=y_range,\n",
        "                ratio=r,\n",
        "            )\n",
        "            g_cali_scores.append(gc)\n",
        "        g_cali_scores = np.array(g_cali_scores)\n",
        "\n",
        "    return (\n",
        "        cali_score,\n",
        "        sharp_score,\n",
        "        obs_props,\n",
        "        quantile_preds,\n",
        "        g_cali_scores,\n",
        "        individual_cali,\n",
        "        pinball_loss,\n",
        "        model_obs,\n",
        "        model_y\n",
        "    )"
      ],
      "metadata": {
        "id": "JNoflVonT7v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load datasets"
      ],
      "metadata": {
        "id": "8SHaoDtLUTKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_bpl(dataset, seed, dataset_path='datasets/UCI_Datasets', extra_val = None):\n",
        "    data = np.loadtxt(\"{}/{}.txt\".format(dataset_path, dataset))\n",
        "    x_al = data[:, :-1]\n",
        "    y_al = data[:, -1].reshape(-1, 1)\n",
        "\n",
        "    x_tr, x_te, y_tr, y_te = train_test_split(\n",
        "        x_al, y_al, test_size=0.1, random_state=seed\n",
        "    )\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(\n",
        "        x_tr, y_tr, test_size=0.2, random_state=seed\n",
        "    )\n",
        "    if extra_val is not None:\n",
        "        x_tr, x_va2, y_tr, y_va2 = train_test_split(\n",
        "            x_tr, y_tr, test_size=extra_val, random_state=seed\n",
        "    )\n",
        "\n",
        "    s_tr_x = StandardScaler().fit(x_tr)\n",
        "    s_tr_y = StandardScaler().fit(y_tr)\n",
        "\n",
        "    x_tr = torch.Tensor(s_tr_x.transform(x_tr))\n",
        "    x_va = torch.Tensor(s_tr_x.transform(x_va))\n",
        "    x_te = torch.Tensor(s_tr_x.transform(x_te))\n",
        "\n",
        "    y_tr = torch.Tensor(s_tr_y.transform(y_tr))\n",
        "    y_va = torch.Tensor(s_tr_y.transform(y_va))\n",
        "    y_te = torch.Tensor(s_tr_y.transform(y_te))\n",
        "    y_al = torch.Tensor(s_tr_y.transform(y_al))\n",
        "\n",
        "    if extra_val is not None:\n",
        "        x_va2 = torch.Tensor(s_tr_x.transform(x_va2))\n",
        "        y_va2 = torch.Tensor(s_tr_y.transform(y_va2))\n",
        "        return x_tr, x_va, x_va2, x_te, y_tr, y_va, y_va2, y_te, y_al\n",
        "\n",
        "    return x_tr, x_va, x_te, y_tr, y_va, y_te, y_al"
      ],
      "metadata": {
        "id": "4y8oZlMSUWg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a base L1 regression model for initializing weights\n",
        "\n",
        "As described in the paper, to help regularize the model, the networks are initalized using the weights obtained via L1 regression on the full training dataset."
      ],
      "metadata": {
        "id": "PYG-Y66XU11J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_model(dataset, seed, path, output_device, extra_val=None, dataset_path='datasets/UCI_Datasets'):\n",
        "    val = True\n",
        "    nepochs = 1000\n",
        "    display = True\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if extra_val is None:\n",
        "        X, X_val, X_test, Y, Y_val, Y_test, Y_al = load_data_bpl(dataset, seed, dataset_path=dataset_path)\n",
        "    else:\n",
        "        X, X_val, X_true_val, X_test, Y, Y_val, Y_true_val, Y_test, Y_al = load_data_bpl(dataset, seed, extra_val=extra_val, dataset_path=dataset_path)\n",
        "\n",
        "    X = X.to(output_device)\n",
        "    Y = Y.to(output_device)\n",
        "    X_val = X_val.to(output_device)\n",
        "    Y_val = Y_val.to(output_device)\n",
        "    X_test = X_test.to(output_device)\n",
        "    Y_test = Y_test.to(output_device)\n",
        "    if not val:\n",
        "        X=torch.cat((X, X_val), dim=0)\n",
        "        Y=torch.cat((Y, Y_val), dim=0)\n",
        "\n",
        "    class data_set(Dataset):\n",
        "        def __init__(self, X, Y):\n",
        "            self.X = X\n",
        "            self.Y = Y\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return self.X[index], self.Y[index]\n",
        "\n",
        "    data = data_set(X=X, Y=Y)\n",
        "    dataloader = DataLoader(data, batch_size=64, shuffle=True)#25\n",
        "\n",
        "    our_model = bpl_nn(X.shape[1]).to(output_device)\n",
        "    optimizer = optim.Adam(our_model.parameters(), lr=1e-3)\n",
        "    loss_fun = torch.nn.L1Loss()\n",
        "\n",
        "    best_loss = torch.inf\n",
        "    best_weights = None\n",
        "    early_stop_count = 0\n",
        "\n",
        "    training_metrics = {'tr': [], 'va': []}\n",
        "\n",
        "    for epoch in (tqdm(range(nepochs)) if display else range(nepochs)):\n",
        "        our_model.train()\n",
        "        batch_loss = []\n",
        "        for Xbatch, Ybatch in dataloader:\n",
        "            Xbatch, Ybatch = Xbatch.to(output_device), Ybatch.to(output_device)\n",
        "            pred = our_model(Xbatch)\n",
        "            loss = loss_fun(pred,Ybatch)\n",
        "            batch_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        training_metrics['tr'].append(np.mean(batch_loss))\n",
        "\n",
        "        if val:\n",
        "            our_model.eval()\n",
        "            with torch.no_grad():\n",
        "                Xbatch, Ybatch = X_val.to(output_device), Y_val.to(output_device)\n",
        "                pred = our_model(Xbatch)\n",
        "                loss = loss_fun(pred,Ybatch)\n",
        "                if loss < best_loss:\n",
        "                    early_stop_count = 0\n",
        "                    best_loss = loss\n",
        "                    best_weights = copy.deepcopy(our_model.state_dict())\n",
        "                else:\n",
        "                    early_stop_count += 1\n",
        "            if early_stop_count > 200:\n",
        "                break\n",
        "            training_metrics['va'].append(loss.item())\n",
        "\n",
        "    if val:\n",
        "        our_model.load_state_dict(best_weights)\n",
        "\n",
        "    torch.save(our_model.state_dict(), path)\n",
        "\n",
        "    return copy.deepcopy(our_model.state_dict())\n"
      ],
      "metadata": {
        "id": "mK1e47GcU5oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluation of CaliPSo model"
      ],
      "metadata": {
        "id": "qZWhmsxvTaw3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdLQHQRz9l3Y"
      },
      "outputs": [],
      "source": [
        "config_path = '.'\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "# Regularization helps prevent overfitting to outliers, particularly for outer quantiles\n",
        "class param_scheduler:\n",
        "    def __init__(self, update_rate: int = 50, decay_rate: float = 0.5, param0: float = 1.0):\n",
        "        self.update_rate = update_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.param = param0\n",
        "        self.counter = 0\n",
        "    def step(self):\n",
        "        self.counter += 1\n",
        "        if self.counter > self.update_rate:\n",
        "            self.counter = 0\n",
        "            self.param *= self.decay_rate\n",
        "        return self.param\n",
        "\n",
        "# Includes several hyperparameters, running the notebook as is will match the configuration reported in the paper.\n",
        "# dataset can be one of: 'yacht','boston','concrete','energy','kin8nm','power','wine','naval','protein'\n",
        "# val_recal: Should you recalibrate on a held-out validation set\n",
        "# display: Whether to output model evaluation metrics\n",
        "# dims: If >0, applies PCA to the input data with n_components=dims\n",
        "# vanilla_weights_path: specifies path to L1 weights\n",
        "# balanced_recal: when true, upsamples the heldout validation set to have the same number of samples as the training set when recalibrating the model\n",
        "# va_split: defines a validation size, as a fraction of the train+val size\n",
        "# cali_favoured: when using early stopping based on the best weighted sum of ECE and sharpness, cali_favoured is the weight of the ECE (sharpness has a fixed weight of 1)\n",
        "# fix_cali: if true, stops based on the ECE achieving a satisfactory level (relative to the Beyond Pinball Loss paper's reported MAQR results), else use the best weighted sum as early stopping criterion\n",
        "def run_experiment(rep, output_device, nepochs=1000, dataset='boston',val_recal=True,seed=0,val=False,display=True,dims=0,vanilla_weights_path=None,balanced_recal=False,va_split=None,cali_favoured=1,fix_cali=False):\n",
        "    argdataset = dataset\n",
        "    np.random.seed(rep)\n",
        "    torch.manual_seed(rep)\n",
        "\n",
        "    if fix_cali:\n",
        "        X, X_val, X_true_val, X_test, Y, Y_val, Y_true_val, Y_test, Y_al = load_data_bpl(dataset, rep, extra_val=0.14)\n",
        "        print([i.shape for i in (X, X_val, X_true_val, X_test, Y, Y_val, Y_true_val, Y_test, Y_al)])\n",
        "    else:\n",
        "        X, X_val, X_test, Y, Y_val, Y_test, Y_al = load_data_bpl(dataset, rep)\n",
        "\n",
        "    if dims > 0:\n",
        "        pca = PCA(n_components=dims)\n",
        "        X = torch.Tensor(pca.fit_transform(X))\n",
        "        X_val = torch.Tensor(pca.transform(X_val))\n",
        "        X_test = torch.Tensor(pca.transform(X_test))\n",
        "\n",
        "    X = X.to(output_device)\n",
        "    Y = Y.to(output_device)\n",
        "    X_val = X_val.to(output_device)\n",
        "    Y_val = Y_val.to(output_device)\n",
        "    X_test = X_test.to(output_device)\n",
        "    Y_test = Y_test.to(output_device)\n",
        "    if fix_cali:\n",
        "        X_true_val = X_true_val.to(output_device)\n",
        "        Y_true_val = Y_true_val.to(output_device)\n",
        "\n",
        "    if not val:\n",
        "        X=torch.cat((X, X_val), dim=0)\n",
        "        Y=torch.cat((Y, Y_val), dim=0)\n",
        "\n",
        "    if va_split is not None:\n",
        "        X = torch.cat((X, X_val), dim=0)\n",
        "        Y = torch.cat((Y, Y_val), dim=0)\n",
        "        split_i = int(len(X) * (1-va_split))\n",
        "        X_val = X[split_i:]\n",
        "        Y_val = Y[split_i:]\n",
        "        X = X[:split_i]\n",
        "        Y = Y[:split_i]\n",
        "        print(len(X), len(X_val))\n",
        "\n",
        "    te_exp_props = torch.linspace(0.01, 0.99, 99).to(output_device)\n",
        "    y_range = (Y_al.max() - Y_al.min()).item()\n",
        "\n",
        "    class data_set(Dataset):\n",
        "        def __init__(self, X, Y):\n",
        "            self.X = X\n",
        "            self.Y = Y\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return self.X[index], self.Y[index]\n",
        "\n",
        "    dataset = data_set(X=X, Y=Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    half_q_levels = torch.tensor([0, 0.025, 0.05, 0.1, 0.2])\n",
        "    lambda_reg_vec = torch.tensor([0.05, 0.005, 0.005, 0.005, 0.005])\n",
        "    scheduler = param_scheduler()\n",
        "    our_model = quantile_model_ensemble(X=X, Y=Y, vanilla_model=bpl_nn, half_q_levels=half_q_levels, output_device=output_device, vanilla_weights_path=[vanilla_weights_path, vanilla_weights_path, None, None, None])\n",
        "    optimizer = optim.Adam(our_model.parameters(), lr=1e-3)\n",
        "    loss_fun = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "    our_model.train()\n",
        "\n",
        "    best_loss = torch.inf\n",
        "    best_weights = None\n",
        "    early_stop_count = 0\n",
        "    best_metric = torch.inf\n",
        "    best_metric_epoch = None\n",
        "    best_sharp = torch.inf\n",
        "    best_sharp_epoch = None\n",
        "    maqr_ece = {\n",
        "        'yacht':   (6.8+2.1*2)/100,\n",
        "        'boston':  (6.2+1.8*2)/100,\n",
        "        'kin8nm':  (1.8+0.4*2)/100,\n",
        "        'energy':  (3.5+1.0*2)/100,\n",
        "        'concrete':(5.3+0.4*2)/100,\n",
        "        'wine':    (2.7+0.2*2)/100,\n",
        "        'power':   (1.6+0.3*2)/100,\n",
        "        'naval':   (2.3+0.2*2)/100,\n",
        "        'protein': (2.6+0.3*2)/100,\n",
        "        }\n",
        "\n",
        "    training_metrics = {metric: {'tr': [], 'va': [], 'te': []} for metric in ['loss', 'cali', 'sharp', 'g_cali', 'ind_cali', 'pinball_loss', 'model_obs', 'model_y']}\n",
        "\n",
        "    for epoch in (tqdm(range(nepochs)) if display else range(nepochs)):\n",
        "        our_model.train()\n",
        "        batch_loss = []\n",
        "        lambda_reg_vec *= scheduler.step()\n",
        "        for Xbatch, Ybatch in dataloader:\n",
        "            Xbatch, Ybatch = Xbatch.to(output_device), Ybatch.to(output_device)\n",
        "            quantile_preds = our_model(Xbatch)\n",
        "            loss = loss_fun(quantile_preds,Ybatch.repeat(1, quantile_preds.shape[1]))\n",
        "            our_model.eval()\n",
        "            quantile_preds_val = our_model(X_val)\n",
        "            diff_val = quantile_preds_val - Y_val\n",
        "            loss_ece = 0\n",
        "            total_q_levels = torch.hstack([half_q_levels, 1-half_q_levels.flip(0)]).to(output_device)\n",
        "            for q in torch.arange(total_q_levels.shape[0]):\n",
        "                loss_ece += torch.quantile(diff_val[:, q], total_q_levels.flip(0)[q])**2\n",
        "            our_model.train()\n",
        "            loss += loss_ece\n",
        "            for i in range(len(half_q_levels)):\n",
        "                lambda_reg = lambda_reg_vec[i]\n",
        "                if lambda_reg != 0:\n",
        "                    for params in our_model.lower_quantile_models[i].parameters():\n",
        "                        loss += lambda_reg*params.norm()\n",
        "                    for params in our_model.upper_quantile_models[i].parameters():\n",
        "                        loss += lambda_reg*params.norm()\n",
        "            batch_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        our_model.eval()\n",
        "        training_metrics['loss']['tr'].append(np.mean(batch_loss))\n",
        "        tr_cali_score,tr_sharp_score,_,_,tr_g_cali_scores, tr_indiv_cali_score, pinball_loss,model_obs,model_y = test_uq(our_model,X,Y,\n",
        "            te_exp_props,y_range,recal_model=None,recal_type=None,test_group_cal=False,)\n",
        "        training_metrics['cali']['tr'].append(tr_cali_score)\n",
        "        training_metrics['sharp']['tr'].append(tr_sharp_score)\n",
        "        training_metrics['g_cali']['tr'].append(tr_g_cali_scores)\n",
        "        training_metrics['ind_cali']['tr'].append(tr_indiv_cali_score)\n",
        "        training_metrics['pinball_loss']['tr'].append(pinball_loss)\n",
        "        training_metrics['model_obs']['tr'].append(model_obs)\n",
        "        training_metrics['model_y']['tr'].append(model_y)\n",
        "\n",
        "        if val:\n",
        "            with torch.no_grad():\n",
        "                Xbatch, Ybatch = X_val.to(output_device), Y_val.to(output_device)\n",
        "                quantile_preds = our_model(Xbatch)\n",
        "                loss = loss_fun(quantile_preds,Ybatch.repeat(1, quantile_preds.shape[1]))\n",
        "                if loss < best_loss:\n",
        "                    best_loss = loss\n",
        "                    best_loss_epoch = epoch\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            training_metrics['loss']['va'].append(loss.item())\n",
        "            if fix_cali:\n",
        "                va_cali_score,va_sharp_score,_,_,va_g_cali_scores,va_indiv_cali_score,pinball_loss,model_obs,model_y = test_uq(our_model,X_true_val,Y_true_val,\n",
        "                te_exp_props,y_range,recal_model=None,recal_type=None,test_group_cal=False,)\n",
        "            else:\n",
        "                va_cali_score,va_sharp_score,_,_,va_g_cali_scores,va_indiv_cali_score,pinball_loss,model_obs,model_y = test_uq(our_model,X_val,Y_val,\n",
        "                te_exp_props,y_range,recal_model=None,recal_type=None,test_group_cal=False,)\n",
        "            training_metrics['cali']['va'].append(va_cali_score)\n",
        "            training_metrics['sharp']['va'].append(va_sharp_score)\n",
        "            training_metrics['g_cali']['va'].append(va_g_cali_scores)\n",
        "            training_metrics['ind_cali']['va'].append(va_indiv_cali_score)\n",
        "            training_metrics['pinball_loss']['va'].append(pinball_loss)\n",
        "            training_metrics['model_obs']['va'].append(model_obs)\n",
        "            training_metrics['model_y']['va'].append(model_y)\n",
        "\n",
        "            metric = (va_cali_score*cali_favoured+va_sharp_score)\n",
        "            if metric < best_metric:\n",
        "                if not fix_cali:\n",
        "                    early_stop_count = 0\n",
        "                    best_weights = copy.deepcopy(our_model.state_dict())\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch\n",
        "            elif not fix_cali:\n",
        "                early_stop_count += 1\n",
        "\n",
        "            if early_stop_count > 200:\n",
        "                break\n",
        "\n",
        "            # Compute metrics for test data\n",
        "            Xbatch, Ybatch = X_test.to(output_device), Y_test.to(output_device)\n",
        "            quantile_preds = our_model(X_test.to(output_device))\n",
        "            loss = loss_fun(quantile_preds,Y_test.to(output_device).repeat(1, quantile_preds.shape[1]))\n",
        "\n",
        "            training_metrics['loss']['te'].append(loss.item())\n",
        "            te_cali_score,te_sharp_score,_,_,te_g_cali_scores,te_indiv_cali_score,pinball_loss,model_obs,model_y  = test_uq(our_model,X_test,Y_test,\n",
        "                te_exp_props,y_range,recal_model=None,recal_type=None,test_group_cal=False,)\n",
        "            training_metrics['cali']['te'].append(te_cali_score)\n",
        "            training_metrics['sharp']['te'].append(te_sharp_score)\n",
        "            training_metrics['g_cali']['te'].append(te_g_cali_scores)\n",
        "            training_metrics['ind_cali']['te'].append(te_indiv_cali_score)\n",
        "            training_metrics['pinball_loss']['te'].append(pinball_loss)\n",
        "            training_metrics['model_obs']['te'].append(model_obs)\n",
        "            training_metrics['model_y']['te'].append(model_y)\n",
        "\n",
        "            if fix_cali:\n",
        "                if va_cali_score <= maqr_ece[argdataset] and va_sharp_score < best_sharp:\n",
        "                    early_stop_count = 0\n",
        "                    best_sharp = va_sharp_score\n",
        "                    best_sharp_epoch = epoch\n",
        "                    best_weights = copy.deepcopy(our_model.state_dict())\n",
        "                else:\n",
        "                    early_stop_count += 1\n",
        "\n",
        "    ## RECALIBRATE QUANTILES NOT CORRESPONDING TO ENSEMBLE MEMBERS\n",
        "    if val:\n",
        "        print(\"Stopping epochs:\", best_loss_epoch, best_metric_epoch, best_sharp_epoch)\n",
        "        if best_weights is not None:\n",
        "            our_model.load_state_dict(best_weights)\n",
        "\n",
        "    if balanced_recal:\n",
        "        if fix_cali:\n",
        "            X_val = torch.cat((X_val, X_true_val))\n",
        "            Y_val = torch.cat((Y_val, Y_true_val))\n",
        "        our_model.recalibrate((torch.cat((X_val, X_val, X_val, X_val)),torch.cat((Y_val, Y_val, Y_val, Y_val))) if (val_recal and val) else None)\n",
        "    else:\n",
        "        our_model.recalibrate((X_val,Y_val) if (val_recal and val) else None)\n",
        "\n",
        "    ## OUR MODEL\n",
        "    stats = []\n",
        "    stats.append(run_diagnostics(X_test, Y_test, our_model,display=False))\n",
        "    stats = np.stack(stats, axis=0)\n",
        "    (\n",
        "        te_cali_score,\n",
        "        te_sharp_score,\n",
        "        te_obs_props,\n",
        "        te_q_preds,\n",
        "        te_g_cali_scores,\n",
        "        te_indiv_cali_scores,\n",
        "        pinball_loss, model_obs, model_y,\n",
        "    ) = test_uq(\n",
        "        our_model,\n",
        "        X_test,\n",
        "        Y_test,\n",
        "        te_exp_props,\n",
        "        y_range,\n",
        "        recal_model=None,\n",
        "        recal_type=None,\n",
        "        test_group_cal=True,\n",
        "        display=False\n",
        "    )\n",
        "    if display:\n",
        "        print(\"ECE       :\",te_cali_score)\n",
        "        print(\"Sharp     :\",te_sharp_score)\n",
        "        print(\"Group Cal :\",te_g_cali_scores)\n",
        "        print(\"Indiv Cal:\", te_indiv_cali_scores)\n",
        "        print(\"Pinball   :\",pinball_loss)\n",
        "        print(\"Actual Obs:\",model_obs)\n",
        "        print(\"Mean Y    :\",model_y)\n",
        "        print(np.mean(training_metrics['cali']['va']), np.mean(training_metrics['cali']['te']))\n",
        "    if dims > 0:\n",
        "        savepath = f'{config_path}/results_vis_val/{argdataset}/{str(val_recal)}/{seed}/'\n",
        "        from pathlib import Path\n",
        "        Path(savepath).mkdir(parents=True, exist_ok=True)\n",
        "        with open(savepath + f'vis_{dims}.pkl', 'wb') as f:\n",
        "            pickle.dump(best_weights, f)\n",
        "    return (te_cali_score, te_sharp_score, te_g_cali_scores, te_indiv_cali_scores, pinball_loss, model_obs, model_y), stats, training_metrics, our_model.state_dict()#, (ece, sharpness)\n",
        "\n",
        "\n",
        "def main(datasets, seeds, device):\n",
        "    run_name = 'results'\n",
        "    Path(f'{config_path}/{run_name}/').mkdir(parents=True, exist_ok=True)\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n{seed}\")\n",
        "        for dataset in datasets:\n",
        "            try:\n",
        "                savepath = f'{config_path}/{run_name}/{dataset}/{seed}/' #parallel_1\n",
        "                # if os.path.exists(savepath):\n",
        "                #     print(\"Skipping\", dataset, val_recal, seed)\n",
        "                #     continue\n",
        "                print(f\"{dataset}\")\n",
        "\n",
        "                Path(f'{config_path}/results/{dataset}').mkdir(parents=True, exist_ok=True)\n",
        "                weights_path = f'{config_path}/results/{dataset}/{seed}_extra_val_0.14_l1.pt'\n",
        "                if not os.path.exists(weights_path):\n",
        "                    gen_model(dataset, seed, weights_path, device, extra_val=0.14)\n",
        "                outs = run_experiment(seed, device, nepochs=1000, dataset=dataset, val_recal=True, val=True, display=True, vanilla_weights_path=weights_path, balanced_recal=True, fix_cali=True)\n",
        "\n",
        "                Path(savepath).mkdir(parents=True, exist_ok=True)\n",
        "                with open(savepath + f'_ours_model.pkl', 'wb') as f:\n",
        "                    pickle.dump(outs[:-1], f)\n",
        "\n",
        "                torch.save(outs[-1], savepath + f'state_dict.pt')\n",
        "            except Exception as e:\n",
        "                print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute training and testing for selected datasets and seeds"
      ],
      "metadata": {
        "id": "nWR_rmzcto5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['boston','concrete','energy']#['yacht','boston','concrete','energy','kin8nm','power','wine','naval','protein']\n",
        "seeds = range(5)\n",
        "main(datasets, seeds, 'cuda')"
      ],
      "metadata": {
        "id": "5x-NlfBAtmLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd70733f-a0cf-4e55-cd1a-ad5a8c828c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0\n",
            "boston\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 499/1000 [00:06<00:06, 73.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([313, 13]), torch.Size([91, 13]), torch.Size([51, 13]), torch.Size([51, 13]), torch.Size([313, 1]), torch.Size([91, 1]), torch.Size([51, 1]), torch.Size([51, 1]), torch.Size([506, 1])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 451/1000 [06:29<07:53,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping epochs: 423 95 249\n",
            "ECE       : 0.12060607224702835\n",
            "Sharp     : 0.10497283052775898\n",
            "Group Cal : [0.49137374 0.35387207 0.27689395 0.24813014 0.22658806 0.22013653\n",
            " 0.2177047  0.19702273 0.2052155  0.18910993]\n",
            "Indiv Cal: tensor([-0.2253, -0.2153, -0.2053, -0.1953, -0.2049, -0.1949, -0.2241, -0.2141,\n",
            "        -0.2433, -0.2333, -0.2233, -0.2329, -0.2229, -0.2129, -0.2029, -0.1929,\n",
            "        -0.1829, -0.1729, -0.1629, -0.1529, -0.1429, -0.1329, -0.1229, -0.1129,\n",
            "        -0.1029, -0.1322, -0.1614, -0.1514, -0.1414, -0.1902, -0.1802, -0.1702,\n",
            "        -0.1602, -0.1502, -0.1794, -0.1890, -0.1790, -0.1690, -0.1590, -0.1490,\n",
            "        -0.1390, -0.1290, -0.1190, -0.1090, -0.0990, -0.0890, -0.0986, -0.0886,\n",
            "        -0.0786, -0.0686, -0.0586, -0.0682, -0.0778, -0.0678, -0.0775, -0.0675,\n",
            "        -0.0967, -0.0867, -0.0767, -0.0863, -0.0763, -0.1055, -0.0955, -0.0855,\n",
            "        -0.0755, -0.0655, -0.0555, -0.0455, -0.0355, -0.0255, -0.0155, -0.0055,\n",
            "         0.0045,  0.0145,  0.0245,  0.0345,  0.0445,  0.0545,  0.0449,  0.0549,\n",
            "         0.0649,  0.0749,  0.0849,  0.0949,  0.1049,  0.0953,  0.1053,  0.1153,\n",
            "         0.1253,  0.0961,  0.1061,  0.0965,  0.1065,  0.0969,  0.1069,  0.1169,\n",
            "         0.1269,  0.1369,  0.1469], device='cuda:0')\n",
            "Pinball   : [0.03880756348371506, 0.07772140204906464, 0.09616457670927048, 0.10917028039693832, 0.11873802542686462, 0.1393687129020691, 0.13301806151866913, 0.12369772791862488, 0.10794716328382492, 0.06933299452066422]\n",
            "Actual Obs: [0.2352941334247589, 0.2549019753932953, 0.3333333432674408, 0.3529411852359772, 0.3529411852359772, 0.7254902124404907, 0.7450980544090271, 0.7647058963775635, 0.8039215803146362, 0.843137264251709]\n",
            "Mean Y    : [-0.37902727723121643, -0.32452401518821716, -0.2931985855102539, -0.26730862259864807, -0.24735991656780243, -0.06535808742046356, -0.03956155478954315, -0.020353004336357117, 0.022641383111476898, 0.11926433444023132]\n",
            "0.0996775916629199 0.14108440911029765\n",
            "\n",
            "1\n",
            "boston\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 297/1000 [00:03<00:07, 89.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([313, 13]), torch.Size([91, 13]), torch.Size([51, 13]), torch.Size([51, 13]), torch.Size([313, 1]), torch.Size([91, 1]), torch.Size([51, 1]), torch.Size([51, 1]), torch.Size([506, 1])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 459/1000 [06:41<07:53,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping epochs: 444 188 257\n",
            "ECE       : 0.07656566798686981\n",
            "Sharp     : 0.13125882895887941\n",
            "Group Cal : [0.48758586 0.30934008 0.2312761  0.2176845  0.18943918 0.17917798\n",
            " 0.17229412 0.15690657 0.14734905 0.14215925]\n",
            "Indiv Cal: tensor([-0.1665, -0.1565, -0.1661, -0.1757, -0.1657, -0.1557, -0.1457, -0.1749,\n",
            "        -0.1649, -0.1549, -0.1449, -0.1545, -0.1445, -0.1345, -0.1441, -0.1341,\n",
            "        -0.1437, -0.1337, -0.1237, -0.1137, -0.1233, -0.1133, -0.1229, -0.1325,\n",
            "        -0.1225, -0.1125, -0.1025, -0.0925, -0.0825, -0.0725, -0.0625, -0.0525,\n",
            "        -0.0622, -0.0522, -0.0422, -0.0322, -0.0222, -0.0122, -0.0218, -0.0510,\n",
            "        -0.0606, -0.0506, -0.0406, -0.0502, -0.0402, -0.0302, -0.0202, -0.0298,\n",
            "        -0.0590, -0.0490, -0.0586, -0.0682, -0.0582, -0.0482, -0.0382, -0.0478,\n",
            "        -0.0575, -0.0475, -0.0375, -0.0471, -0.0371, -0.0271, -0.0367, -0.0267,\n",
            "        -0.0167, -0.0263, -0.0163, -0.0063,  0.0037,  0.0137,  0.0237,  0.0337,\n",
            "         0.0437,  0.0537,  0.0637,  0.0345,  0.0053,  0.0153,  0.0253,  0.0353,\n",
            "         0.0453,  0.0553,  0.0653,  0.0753,  0.0853,  0.0953,  0.0857,  0.0957,\n",
            "         0.1057,  0.1157,  0.1257,  0.0769,  0.0673,  0.0773,  0.0676,  0.0776,\n",
            "         0.0876,  0.0976,  0.1076], device='cuda:0')\n",
            "Pinball   : [0.028343573212623596, 0.058222632855176926, 0.07963092625141144, 0.09595370292663574, 0.10987342149019241, 0.12227049469947815, 0.11356961727142334, 0.09755297750234604, 0.06432416290044785, 0.01477696280926466]\n",
            "Actual Obs: [0.19607843458652496, 0.21568627655506134, 0.2549019753932953, 0.27450981736183167, 0.3137255012989044, 0.7254902124404907, 0.7647058963775635, 0.7843137383460999, 0.843137264251709, 0.8823529481887817]\n",
            "Mean Y    : [-0.24292023479938507, -0.18663068115711212, -0.15658926963806152, -0.13552261888980865, -0.11179840564727783, 0.17420987784862518, 0.2241135984659195, 0.27686068415641785, 0.3074153661727905, 0.41148123145103455]\n",
            "0.09389494462748585 0.10181951973270746\n",
            "\n",
            "2\n",
            "boston\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 303/1000 [00:03<00:07, 88.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([313, 13]), torch.Size([91, 13]), torch.Size([51, 13]), torch.Size([51, 13]), torch.Size([313, 1]), torch.Size([91, 1]), torch.Size([51, 1]), torch.Size([51, 1]), torch.Size([506, 1])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▎   | 625/1000 [08:52<05:19,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping epochs: 618 217 423\n",
            "ECE       : 0.09260052442550659\n",
            "Sharp     : 0.12664088626755513\n",
            "Group Cal : [0.47949495 0.35344109 0.25516499 0.24360191 0.22745324 0.17955137\n",
            " 0.18590137 0.17204041 0.17451404 0.1688075 ]\n",
            "Indiv Cal: tensor([-0.2253, -0.2153, -0.2053, -0.2149, -0.2049, -0.2145, -0.2437, -0.2337,\n",
            "        -0.2237, -0.2137, -0.2037, -0.1937, -0.1837, -0.1933, -0.1833, -0.1929,\n",
            "        -0.1829, -0.1729, -0.1825, -0.1725, -0.1625, -0.1722, -0.1622, -0.1522,\n",
            "        -0.1422, -0.1322, -0.1222, -0.1318, -0.1414, -0.1314, -0.1214, -0.1114,\n",
            "        -0.1014, -0.0914, -0.0814, -0.0714, -0.0614, -0.0514, -0.0414, -0.0510,\n",
            "        -0.0410, -0.0310, -0.0210, -0.0110, -0.0206, -0.0106, -0.0006,  0.0094,\n",
            "        -0.0198, -0.0098, -0.0194, -0.0486, -0.0582, -0.0482, -0.0382, -0.0282,\n",
            "        -0.0182, -0.0082, -0.0178, -0.0471, -0.0371, -0.0467, -0.0367, -0.0463,\n",
            "        -0.0363, -0.0263, -0.0163, -0.0063,  0.0037,  0.0137,  0.0237,  0.0141,\n",
            "         0.0241,  0.0341,  0.0441,  0.0541,  0.0445,  0.0545,  0.0645,  0.0549,\n",
            "         0.0649,  0.0749,  0.0653,  0.0753,  0.0853,  0.0757,  0.0857,  0.0957,\n",
            "         0.1057,  0.1157,  0.1257,  0.1161,  0.1261,  0.1165,  0.0676,  0.0580,\n",
            "         0.0680,  0.0780,  0.0880], device='cuda:0')\n",
            "Pinball   : [0.02878362312912941, 0.06370145082473755, 0.08027402311563492, 0.09551192820072174, 0.10606251657009125, 0.11958159506320953, 0.11213693767786026, 0.09386642277240753, 0.07180142402648926, 0.02983762137591839]\n",
            "Actual Obs: [0.29411765933036804, 0.3137255012989044, 0.3137255012989044, 0.3529411852359772, 0.3921568691730499, 0.7254902124404907, 0.7843137383460999, 0.7843137383460999, 0.7843137383460999, 0.9019607901573181]\n",
            "Mean Y    : [-0.31076306104660034, -0.2552901804447174, -0.22897277772426605, -0.20837178826332092, -0.18806099891662598, 0.05192502215504646, 0.09538087993860245, 0.11458135396242142, 0.13907462358474731, 0.27335286140441895]\n",
            "0.08274799399077892 0.11078093593418598\n",
            "\n",
            "3\n",
            "boston\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 342/1000 [00:04<00:08, 79.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([313, 13]), torch.Size([91, 13]), torch.Size([51, 13]), torch.Size([51, 13]), torch.Size([313, 1]), torch.Size([91, 1]), torch.Size([51, 1]), torch.Size([51, 1]), torch.Size([506, 1])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 649/1000 [09:25<05:05,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping epochs: 618 161 447\n",
            "ECE       : 0.10099821537733078\n",
            "Sharp     : 0.10584715961248406\n",
            "Group Cal : [0.49459596 0.35019529 0.26053872 0.22044683 0.21495257 0.19809022\n",
            " 0.18113547 0.18966162 0.17507744 0.16402654]\n",
            "Indiv Cal: tensor([-0.1861, -0.1761, -0.1661, -0.1561, -0.1461, -0.1361, -0.1457, -0.1357,\n",
            "        -0.1257, -0.1353, -0.1449, -0.1349, -0.1249, -0.1149, -0.1049, -0.0949,\n",
            "        -0.0849, -0.0749, -0.0649, -0.0549, -0.0449, -0.0349, -0.0445, -0.0345,\n",
            "        -0.0245, -0.0145, -0.0045,  0.0055,  0.0155,  0.0255,  0.0355,  0.0455,\n",
            "         0.0555,  0.0459,  0.0559,  0.0659,  0.0563,  0.0467,  0.0567,  0.0667,\n",
            "         0.0178,  0.0278,  0.0378,  0.0086,  0.0186,  0.0090,  0.0190,  0.0290,\n",
            "         0.0194,  0.0294,  0.0394,  0.0494,  0.0594,  0.0694,  0.0598,  0.0698,\n",
            "         0.0798,  0.0898,  0.0802,  0.0902,  0.1002,  0.1102,  0.1006,  0.1106,\n",
            "         0.1206,  0.1110,  0.1210,  0.1310,  0.1018,  0.1118,  0.1218,  0.1318,\n",
            "         0.1418,  0.1518,  0.1225,  0.1325,  0.1425,  0.1329,  0.1429,  0.1529,\n",
            "         0.1629,  0.1533,  0.1437,  0.1537,  0.1637,  0.1737,  0.1837,  0.1937,\n",
            "         0.1841,  0.1941,  0.2041,  0.1945,  0.2045,  0.1753,  0.1461,  0.1561,\n",
            "         0.1661,  0.1761,  0.1861], device='cuda:0')\n",
            "Pinball   : [0.050972919911146164, 0.07663695514202118, 0.09450645744800568, 0.10644778609275818, 0.11343824863433838, 0.10647571831941605, 0.09507555514574051, 0.08099345117807388, 0.06359106302261353, 0.03255505487322807]\n",
            "Actual Obs: [0.19607843458652496, 0.21568627655506134, 0.2549019753932953, 0.2549019753932953, 0.27450981736183167, 0.6470588445663452, 0.686274528503418, 0.7058823704719543, 0.7450980544090271, 0.8039215803146362]\n",
            "Mean Y    : [-0.2812361419200897, -0.23009943962097168, -0.1890818029642105, -0.1667226254940033, -0.14623889327049255, 0.06067555770277977, 0.09153016656637192, 0.12483982741832733, 0.1433594822883606, 0.2168910950422287]\n",
            "0.08880836458733449 0.0870654805289303\n",
            "\n",
            "4\n",
            "boston\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 616/1000 [00:07<00:04, 81.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([313, 13]), torch.Size([91, 13]), torch.Size([51, 13]), torch.Size([51, 13]), torch.Size([313, 1]), torch.Size([91, 1]), torch.Size([51, 1]), torch.Size([51, 1]), torch.Size([506, 1])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 306/1000 [04:19<09:47,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping epochs: 306 104 104\n",
            "ECE       : 0.10713012516498566\n",
            "Sharp     : 0.1450740498252923\n",
            "Group Cal : [0.47612121 0.33688217 0.28115152 0.24860073 0.22577954 0.20873528\n",
            " 0.18084611 0.19314395 0.17890686 0.17426224]\n",
            "Indiv Cal: tensor([-0.1665, -0.1565, -0.1465, -0.1365, -0.1265, -0.1361, -0.1849, -0.1945,\n",
            "        -0.1845, -0.1941, -0.2037, -0.2133, -0.2229, -0.2129, -0.2225, -0.2322,\n",
            "        -0.2222, -0.2122, -0.2022, -0.1922, -0.1822, -0.1918, -0.2014, -0.1914,\n",
            "        -0.2010, -0.1910, -0.1810, -0.1710, -0.1610, -0.1510, -0.1410, -0.1310,\n",
            "        -0.1602, -0.1698, -0.1598, -0.1694, -0.1594, -0.1690, -0.1590, -0.1490,\n",
            "        -0.1390, -0.1486, -0.1386, -0.1286, -0.1186, -0.1086, -0.0986, -0.1082,\n",
            "        -0.0982, -0.0882, -0.0782, -0.0682, -0.0582, -0.0482, -0.0382, -0.0282,\n",
            "        -0.0182, -0.0278, -0.0178, -0.0078,  0.0022,  0.0122,  0.0025, -0.0071,\n",
            "        -0.0167, -0.0067, -0.0163, -0.0063,  0.0037, -0.0059,  0.0041,  0.0141,\n",
            "         0.0241,  0.0341,  0.0441,  0.0541,  0.0641,  0.0545,  0.0645,  0.0549,\n",
            "         0.0453,  0.0553,  0.0653,  0.0753,  0.0853,  0.0757,  0.0857,  0.0957,\n",
            "         0.0861,  0.0961,  0.0865,  0.0769,  0.0673,  0.0773,  0.0676,  0.0580,\n",
            "         0.0680,  0.0584,  0.0684], device='cuda:0')\n",
            "Pinball   : [0.043628592044115067, 0.07586360722780228, 0.09675702452659607, 0.11156457662582397, 0.12196970731019974, 0.1406339854001999, 0.13075898587703705, 0.10738152265548706, 0.07805430144071579, 0.03829335793852806]\n",
            "Actual Obs: [0.1764705926179886, 0.2549019753932953, 0.3137255012989044, 0.3529411852359772, 0.4313725531101227, 0.7058823704719543, 0.7843137383460999, 0.8039215803146362, 0.8627451062202454, 0.9215686321258545]\n",
            "Mean Y    : [-0.2638357877731323, -0.1760280877351761, -0.14319837093353271, -0.12144516408443451, -0.0849740281701088, 0.157028466463089, 0.22303406894207, 0.2581421434879303, 0.2975282669067383, 0.4126349985599518]\n",
            "0.11832548936386061 0.13441626355051994\n"
          ]
        }
      ]
    }
  ]
}