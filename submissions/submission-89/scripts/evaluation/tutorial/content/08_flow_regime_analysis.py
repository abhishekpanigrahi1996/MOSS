"""
GMM Model Evaluation Tutorial - Flow Regime Analysis
====================================================

This tutorial demonstrates comprehensive analysis of entropy and log-Wasserstein distance
dependence on flow speed across different architectural regimes and base models.

Topics covered:
1. Flow injection vs flow substitution comparison
2. Direct vs fractional flow distribution modes
3. Entropy and clustering quality relationships
4. Cross-model architecture analysis
5. Statistical correlation patterns

This tutorial compares:
- No-Flow Model (manual flow injection)
- Baseline Model (flow substitution)
- Direct vs Fractional flow distribution modes
- Flow ranges 0â†’1 and 0â†’5
"""

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import copy
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from scipy import stats

# Add project root to path
project_root = '/mount/Storage/gmm-v4/'
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Set device
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Import utilities
from scripts.evaluation.tutorial.src.io import (
    load_model_from_experiment,
    create_data_samples
)
from scripts.evaluation.tutorial.src.eval_utils import (
    knn_entropy_bias_reduced_torch
)
from scripts.evaluation.tutorial.src.visualization import (
    set_plotting_style
)
from losses.wasserstein import wasserstein_loss

# Set global plotting style
set_plotting_style()

# Define paths
output_dir = Path('/mount/Storage/gmm-v4/output')
experiment_base_dir = output_dir / 'final_experiments'
tutorial_output_dir = Path('/mount/Storage/gmm-v4/scripts/evaluation/tutorial/output')
tutorial_output_dir.mkdir(parents=True, exist_ok=True)

# Create subdirectory for this tutorial
flow_analysis_output_dir = tutorial_output_dir / 'flow_regime_analysis'
flow_analysis_output_dir.mkdir(parents=True, exist_ok=True)

def create_flow_regime_models(base_model_path, repeat_factor_multiplier=1):
    """
    Create direct and fractional flow distribution models from a base model.
    
    Args:
        base_model_path: Path to the base model experiment
        repeat_factor_multiplier: Multiplier for repeat factor (for higher flow ranges)
        
    Returns:
        tuple: (direct_model, fractional_model, config)
    """
    print(f"\n=== Creating Flow Regime Models ===")
    print(f"Base model: {base_model_path}")
    print(f"Repeat factor multiplier: {repeat_factor_multiplier}")
    
    # Load the base model
    base_model, config = load_model_from_experiment(base_model_path, load_best=False, device=device)
    base_model.eval()
    
    print(f"Base model loaded:")
    print(f"  Flow enabled: {base_model.transformer.use_flow_predictor}")
    print(f"  Current repeat factor: {base_model.transformer.repeat_factor}")
    print(f"  Current flow distribution mode: {getattr(base_model.transformer, 'flow_distribution_mode', 'direct')}")
    
    # Create direct flow model (copy of base)
    direct_model = copy.deepcopy(base_model)
    direct_model.transformer.use_flow_predictor = False  # We'll control flow manually
    direct_model.transformer.flow_distribution_mode = "direct"
    
    # Apply repeat factor multiplier if needed
    if repeat_factor_multiplier > 1:
        original_repeat = direct_model.transformer.repeat_factor
        new_repeat = original_repeat * repeat_factor_multiplier
        direct_model.transformer.repeat_factor = new_repeat
        print(f"  Direct model repeat factor: {original_repeat} â†’ {new_repeat}")
    
    # Create fractional flow model (copy of base)
    fractional_model = copy.deepcopy(base_model)
    fractional_model.transformer.use_flow_predictor = False  # We'll control flow manually
    fractional_model.transformer.flow_distribution_mode = "fractional"
    
    # Apply repeat factor multiplier if needed
    if repeat_factor_multiplier > 1:
        original_repeat = fractional_model.transformer.repeat_factor
        new_repeat = original_repeat * repeat_factor_multiplier
        fractional_model.transformer.repeat_factor = new_repeat
        print(f"  Fractional model repeat factor: {original_repeat} â†’ {new_repeat}")
    
    print("\nâœ“ Flow regime models created:")
    print(f"  Direct model: flow_distribution_mode = {direct_model.transformer.flow_distribution_mode}")
    print(f"  Fractional model: flow_distribution_mode = {fractional_model.transformer.flow_distribution_mode}")
    print(f"  Both models have repeat_factor = {direct_model.transformer.repeat_factor}")
    
    return direct_model, fractional_model, config

def create_evaluation_data(model_dtype, num_samples=10, points_per_sample=1000):
    """
    Create evaluation data for flow regime analysis.
    
    Args:
        model_dtype: Data type to match model
        num_samples: Number of different GMM instances to create
        points_per_sample: Number of points per GMM instance
        
    Returns:
        tuple: (inputs, targets)
    """
    print(f"\n=== Creating Evaluation Data ===")
    print(f"Generating {num_samples} samples with {points_per_sample} points each")
    
    # Create data samples
    data = create_data_samples(
        dataset_name='standard',
        num_samples=num_samples,
        points_per_gmm=points_per_sample,
        device=device
    )
    
    # Unpack data
    inputs, targets = data
    
    # Convert to match model dtype
    inputs = inputs.to(model_dtype)
    targets = {k: v.to(model_dtype) if isinstance(v, torch.Tensor) else v 
               for k, v in targets.items()}
    
    # Extract key information
    centers = targets['centers']
    labels = targets['labels']
    n_clusters = centers.shape[1]
    
    print(f"âœ“ Data created:")
    print(f"  Input shape: {inputs.shape}")
    print(f"  Centers shape: {centers.shape}")
    print(f"  Number of clusters: {n_clusters}")
    
    return inputs, targets

def evaluate_flow_regime(model, model_name, inputs, targets, flow_speeds, 
                        repeat_factor_multiplier=1):
    """
    Evaluate a single flow regime model across different flow speeds.
    
    Args:
        model: The model to evaluate
        model_name: Name for this model (e.g., "Direct", "Fractional")
        inputs: Input data [num_samples, num_points, dim]
        targets: Target data dictionary
        flow_speeds: Array of flow speeds to evaluate
        repeat_factor_multiplier: Multiplier for flow speeds (for both modes when using expanded layers)
        
    Returns:
        dict: Results containing flow speeds, entropy values, and Wasserstein distances
    """
    print(f"\n=== Evaluating {model_name} Model ===")
    print(f"Flow distribution mode: {model.transformer.flow_distribution_mode}")
    print(f"Repeat factor: {model.transformer.repeat_factor}")
    print(f"Flow speed range: {flow_speeds.min():.3f} to {flow_speeds.max():.3f}")
    print(f"Number of flow speeds: {len(flow_speeds)}")
    
    entropy_values = []
    wasserstein_values = []
    
    # Process each flow speed
    for i, flow_speed in enumerate(tqdm(flow_speeds, desc=f"Evaluating {model_name}")):
        
        # For both modes with multiplied layers, divide flow by multiplier
        # This ensures equivalent computation budget when both models have multiplied layers
        if repeat_factor_multiplier > 1:
            actual_flow_value = flow_speed / repeat_factor_multiplier
        else:
            actual_flow_value = flow_speed
        
        flow_tensor = torch.tensor([actual_flow_value], device=device, dtype=inputs.dtype)
        
        # Run model inference
        with torch.no_grad():
            # Use a single sample for analysis
            sample_input = inputs[0:1]  # [1, num_points, dim]
            sample_centers = targets['centers'][0:1]  # [1, n_clusters, dim]
            sample_labels = targets['labels'][0:1].long()  # [1, num_points] - ensure int64 dtype
            
            # Get model predictions
            predictions = model(sample_input, flow_speed=flow_tensor)
            
            # Compute entropy
            entropy = knn_entropy_bias_reduced_torch(
                predictions[0],  # Remove batch dimension
                k=5,
                device='cpu',
                subsample='random',
                B=3,
                seed=42
            )
            entropy_values.append(entropy)
            
            # Compute Wasserstein distance
            wasserstein_dist = wasserstein_loss(
                predictions=predictions,
                labels=sample_labels,
                positions=sample_centers,
                implementation="pot",
                algorithm="exact",
                reduction="mean"
            ).item()
            
            # Store log Wasserstein distance (clamp to avoid log(0))
            log_wasserstein = np.log(max(wasserstein_dist, 1e-10))
            wasserstein_values.append(log_wasserstein)
        
        # Debug output for first few values
        if i < 3:
            print(f"  Flow {flow_speed:.3f} â†’ actual {actual_flow_value:.3f}: "
                  f"entropy={entropy:.4f}, log_wasserstein={log_wasserstein:.4f}")
    
    print(f"âœ“ {model_name} evaluation complete")
    print(f"  Entropy range: {min(entropy_values):.4f} to {max(entropy_values):.4f}")
    print(f"  Log Wasserstein range: {min(wasserstein_values):.4f} to {max(wasserstein_values):.4f}")
    
    return {
        'model_name': model_name,
        'flow_speeds': flow_speeds.cpu().numpy(),
        'entropy_values': entropy_values,
        'wasserstein_values': wasserstein_values,
        'flow_distribution_mode': model.transformer.flow_distribution_mode,
        'repeat_factor': model.transformer.repeat_factor
    }

def create_comparison_plots(results_list, base_model_name, flow_range_name="0_to_1", save_plots=True):
    """
    Create comparison plots for entropy and log-Wasserstein distance vs flow speed.
    
    Args:
        results_list: List of results dictionaries from evaluate_flow_regime
        base_model_name: Name of the base model (e.g., "no_flow_16_layers")
        flow_range_name: Name for the flow range (used in filenames)
        save_plots: Whether to save plots to files
        
    Returns:
        tuple: (entropy_fig, wasserstein_fig, combined_fig)
    """
    print(f"\n=== Creating Comparison Plots ===")
    print(f"Comparing {len(results_list)} flow regimes for {base_model_name}")
    
    # Set up the plotting style
    plt.style.use('default')
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    # Create individual plots
    fig_entropy, ax_entropy = plt.subplots(1, 1, figsize=(10, 6))
    fig_wasserstein, ax_wasserstein = plt.subplots(1, 1, figsize=(10, 6))
    
    # Create combined plot with dual y-axis
    fig_combined, ax1 = plt.subplots(1, 1, figsize=(12, 8))
    ax2 = ax1.twinx()
    
    # Plot each regime
    for i, results in enumerate(results_list):
        color = colors[i % len(colors)]
        model_name = results['model_name']
        flow_speeds = results['flow_speeds']
        entropy_values = results['entropy_values']
        wasserstein_values = results['wasserstein_values']
        
        # Entropy plot
        ax_entropy.plot(flow_speeds, entropy_values, 'o-', 
                       color=color, linewidth=2, markersize=4,
                       label=f"{model_name} (mode: {results['flow_distribution_mode']})")
        
        # Wasserstein plot
        ax_wasserstein.plot(flow_speeds, wasserstein_values, 's-', 
                           color=color, linewidth=2, markersize=4,
                           label=f"{model_name} (mode: {results['flow_distribution_mode']})")
        
        # Combined plot
        ax1.plot(flow_speeds, entropy_values, 'o-', 
                color=color, linewidth=2, markersize=4,
                label=f"{model_name} Entropy")
        ax2.plot(flow_speeds, wasserstein_values, 's--', 
                color=color, linewidth=2, markersize=4,
                label=f"{model_name} Log Wasserstein")
    
    # Configure entropy plot
    ax_entropy.set_xlabel('Flow Speed', fontsize=12)
    ax_entropy.set_ylabel('Entropy (nats)', fontsize=12)
    ax_entropy.set_title(f'Distribution Entropy vs Flow Speed\n({base_model_name})', fontsize=14, fontweight='bold')
    ax_entropy.grid(True, alpha=0.3)
    ax_entropy.legend(fontsize=10)
    
    # Configure Wasserstein plot
    ax_wasserstein.set_xlabel('Flow Speed', fontsize=12)
    ax_wasserstein.set_ylabel('Log Wasserstein Distance', fontsize=12)
    ax_wasserstein.set_title(f'Log Wasserstein Distance vs Flow Speed\n({base_model_name})', fontsize=14, fontweight='bold')
    ax_wasserstein.grid(True, alpha=0.3)
    ax_wasserstein.legend(fontsize=10)
    
    # Configure combined plot
    ax1.set_xlabel('Flow Speed', fontsize=12)
    ax1.set_ylabel('Entropy (nats)', color='black', fontsize=12)
    ax2.set_ylabel('Log Wasserstein Distance', color='black', fontsize=12)
    ax1.set_title(f'Entropy and Log Wasserstein Distance vs Flow Speed\n{base_model_name} - Direct vs Fractional Flow Distribution', 
                  fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # Combine legends for the dual-axis plot
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=10)
    
    # Adjust layouts
    fig_entropy.tight_layout()
    fig_wasserstein.tight_layout()
    fig_combined.tight_layout()
    
    # Save plots if requested
    if save_plots:
        # Create unique output names
        output_suffix = f"{base_model_name}_{flow_range_name}"
        
        # Save individual plots
        entropy_path = flow_analysis_output_dir / f'entropy_vs_flow_{output_suffix}.png'
        wasserstein_path = flow_analysis_output_dir / f'log_wasserstein_vs_flow_{output_suffix}.png'
        combined_path = flow_analysis_output_dir / f'combined_metrics_vs_flow_{output_suffix}.png'
        
        fig_entropy.savefig(entropy_path, dpi=300, bbox_inches='tight')
        fig_wasserstein.savefig(wasserstein_path, dpi=300, bbox_inches='tight')
        fig_combined.savefig(combined_path, dpi=300, bbox_inches='tight')
        
        print(f"âœ“ Plots saved:")
        print(f"  Entropy: {entropy_path}")
        print(f"  Log Wasserstein: {wasserstein_path}")
        print(f"  Combined: {combined_path}")
    
    # Show plots
    plt.show()
    
    return fig_entropy, fig_wasserstein, fig_combined

def create_correlation_analysis(results_list, base_model_name, flow_range_name="0_to_1", save_plots=True):
    """
    Create correlation analysis between entropy and log-Wasserstein distance.
    
    Args:
        results_list: List of results dictionaries
        base_model_name: Name of the base model
        flow_range_name: Name for the flow range
        save_plots: Whether to save plots
        
    Returns:
        matplotlib figure
    """
    print(f"\n=== Creating Correlation Analysis ===")
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    for i, results in enumerate(results_list):
        color = colors[i % len(colors)]
        model_name = results['model_name']
        flow_speeds = results['flow_speeds']
        entropy_values = results['entropy_values']
        wasserstein_values = results['wasserstein_values']
        
        # Create scatter plot with flow speed as color
        scatter = ax.scatter(entropy_values, wasserstein_values, 
                           c=flow_speeds, cmap='viridis', 
                           s=50, alpha=0.7, edgecolors=color, linewidth=1,
                           label=f"{model_name} ({results['flow_distribution_mode']})")
        
        # Add trend line
        z = np.polyfit(entropy_values, wasserstein_values, 1)
        p = np.poly1d(z)
        entropy_sorted = sorted(entropy_values)
        ax.plot(entropy_sorted, p(entropy_sorted), '--', color=color, alpha=0.8, linewidth=2)
        
        # Calculate correlation
        correlation = np.corrcoef(entropy_values, wasserstein_values)[0, 1]
        print(f"  {model_name} correlation: {correlation:.4f}")
    
    # Configure plot
    ax.set_xlabel('Entropy (nats)', fontsize=12)
    ax.set_ylabel('Log Wasserstein Distance', fontsize=12)
    ax.set_title(f'Correlation: Entropy vs Log Wasserstein Distance\n{base_model_name} (Colored by Flow Speed)', 
                 fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=10)
    
    # Add colorbar
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Flow Speed', fontsize=10)
    
    plt.tight_layout()
    
    # Save plot if requested
    if save_plots:
        output_suffix = f"{base_model_name}_{flow_range_name}"
        correlation_path = flow_analysis_output_dir / f'entropy_wasserstein_correlation_{output_suffix}.png'
        fig.savefig(correlation_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Correlation plot saved: {correlation_path}")
    
    plt.show()
    return fig

def save_results_to_csv(results_list, base_model_name, flow_range_name="0_to_1"):
    """
    Save numerical results to CSV files for further analysis.
    
    Args:
        results_list: List of results dictionaries
        base_model_name: Name of the base model
        flow_range_name: Name for the flow range
    """
    print(f"\n=== Saving Results to CSV ===")
    
    # Combine all results into a single DataFrame
    all_data = []
    
    for results in results_list:
        model_name = results['model_name']
        flow_speeds = results['flow_speeds']
        entropy_values = results['entropy_values']
        wasserstein_values = results['wasserstein_values']
        flow_mode = results['flow_distribution_mode']
        repeat_factor = results['repeat_factor']
        
        for flow, entropy, wasserstein in zip(flow_speeds, entropy_values, wasserstein_values):
            all_data.append({
                'base_model': base_model_name,
                'model_name': model_name,
                'flow_distribution_mode': flow_mode,
                'repeat_factor': repeat_factor,
                'flow_speed': flow,
                'entropy': entropy,
                'log_wasserstein': wasserstein
            })
    
    # Create DataFrame and save
    df = pd.DataFrame(all_data)
    output_suffix = f"{base_model_name}_{flow_range_name}"
    csv_path = flow_analysis_output_dir / f'flow_regime_analysis_{output_suffix}.csv'
    df.to_csv(csv_path, index=False)
    
    print(f"âœ“ Results saved to: {csv_path}")
    print(f"  Total rows: {len(df)}")
    print(f"  Columns: {list(df.columns)}")
    
    return df

def analyze_single_model(base_experiment_name, base_experiment_desc):
    """Analyze a single base model across all flow ranges."""
    
    print(f"\n{'#'*80}")
    print(f"ANALYZING BASE MODEL: {base_experiment_name}")
    print(f"Description: {base_experiment_desc}")
    print(f"{'#'*80}")
    
    flow_ranges = [
        {'range': (0.0, 1.0), 'name': '0_to_1', 'frames': 50, 'multiplier': 1},
        {'range': (0.0, 5.0), 'name': '0_to_5', 'frames': 50, 'multiplier': 5}
    ]
    
    # Get base model path
    base_model_path = experiment_base_dir / base_experiment_name
    print(f"Using base model: {base_model_path}")
    
    # Process each flow range for this base model
    all_results = {}
    
    for flow_config in flow_ranges:
        flow_range = flow_config['range']
        range_name = flow_config['name']
        num_points = flow_config['frames']
        multiplier = flow_config['multiplier']
        
        print(f"\n{'='*60}")
        print(f"ANALYZING FLOW RANGE: {flow_range[0]} to {flow_range[1]} ({range_name})")
        print(f"Base Model: {base_experiment_name}")
        print(f"{'='*60}")
        
        # Create flow regime models
        direct_model, fractional_model, config = create_flow_regime_models(
            base_model_path, repeat_factor_multiplier=multiplier
        )
        
        # Create evaluation data
        inputs, targets = create_evaluation_data(
            next(direct_model.parameters()).dtype,
            num_samples=5,  # Use multiple samples for robustness
            points_per_sample=1000
        )
        
        # Create flow speed array
        flow_speeds = torch.linspace(flow_range[0], flow_range[1], num_points)
        print(f"Flow speeds: {flow_speeds.min():.3f} to {flow_speeds.max():.3f} ({len(flow_speeds)} points)")
        
        # Evaluate both regimes
        results_list = []
        
        # Evaluate direct model
        direct_results = evaluate_flow_regime(
            direct_model, "Direct", inputs, targets, flow_speeds, multiplier
        )
        results_list.append(direct_results)
        
        # Evaluate fractional model
        fractional_results = evaluate_flow_regime(
            fractional_model, "Fractional", inputs, targets, flow_speeds, multiplier
        )
        results_list.append(fractional_results)
        
        # Store results for cross-model comparison
        all_results[range_name] = results_list
        
        # Create comparison plots with unique names
        create_comparison_plots(results_list, base_experiment_name, range_name, save_plots=True)
        
        # Create correlation analysis with unique names
        create_correlation_analysis(results_list, base_experiment_name, range_name, save_plots=True)
        
        # Save numerical results with unique names
        save_results_to_csv(results_list, base_experiment_name, range_name)
        
        print(f"\nâœ“ Analysis complete for {base_experiment_name} - range {range_name}")
    
    return all_results

def create_comprehensive_summary():
    """Create a comprehensive summary of all results."""
    
    print(f"\n{'='*100}")
    print("COMPREHENSIVE FLOW REGIME ANALYSIS SUMMARY")
    print(f"{'='*100}")
    
    # Load all CSV files and create comprehensive summary
    summary_data = []
    
    base_models = ['no_flow_16_layers', 'baseline_16_layers']
    flow_ranges = ['0_to_1', '0_to_5']
    
    for base_model in base_models:
        for flow_range in flow_ranges:
            csv_path = flow_analysis_output_dir / f'flow_regime_analysis_{base_model}_{flow_range}.csv'
            
            if csv_path.exists():
                df = pd.read_csv(csv_path)
                
                for mode in ['direct', 'fractional']:
                    subset = df[df['flow_distribution_mode'] == mode]
                    
                    if not subset.empty:
                        correlation = subset['entropy'].corr(subset['log_wasserstein'])
                        entropy_range = (subset['entropy'].min(), subset['entropy'].max())
                        wasserstein_range = (subset['log_wasserstein'].min(), subset['log_wasserstein'].max())
                        
                        summary_data.append({
                            'base_model': base_model.replace('_16_layers', ''),
                            'flow_range': flow_range.replace('_', '.'),
                            'distribution_mode': mode,
                            'entropy_min': entropy_range[0],
                            'entropy_max': entropy_range[1],
                            'entropy_range': entropy_range[1] - entropy_range[0],
                            'wasserstein_min': wasserstein_range[0],
                            'wasserstein_max': wasserstein_range[1],
                            'wasserstein_range': wasserstein_range[1] - wasserstein_range[0],
                            'correlation': correlation
                        })
    
    # Create and display summary DataFrame
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        
        print("\nðŸ“‹ Complete Summary Table:")
        print(summary_df.to_string(index=False, float_format='%.4f'))
        
        # Save summary table
        summary_path = flow_analysis_output_dir / 'comprehensive_summary.csv'
        summary_df.to_csv(summary_path, index=False)
        print(f"\nâœ“ Summary table saved to: {summary_path}")
        
        return summary_df
    else:
        print("âŒ No data found for comprehensive summary")
        return None

def main():
    """Main tutorial function demonstrating comprehensive flow regime analysis."""
    print("="*80)
    print("TUTORIAL 08: COMPREHENSIVE FLOW REGIME ANALYSIS")
    print("="*80)
    print("\nThis tutorial demonstrates:")
    print("1. Flow injection vs flow substitution")
    print("2. Direct vs fractional flow distribution modes") 
    print("3. Entropy and clustering quality relationships")
    print("4. Cross-model architecture analysis")
    print("5. Statistical correlation patterns")
    
    # Configuration
    base_experiments = [
        {'name': 'no_flow_16_layers', 'description': 'No-Flow Model (Manual Flow Injection)'},
        {'name': 'baseline_16_layers', 'description': 'Baseline Model (Flow Substitution)'}
    ]
    
    # Analyze each model
    all_model_results = {}
    
    for base_exp in base_experiments:
        experiment_name = base_exp['name']
        experiment_desc = base_exp['description']
        
        model_results = analyze_single_model(experiment_name, experiment_desc)
        all_model_results[experiment_name] = model_results
    
    # Create comprehensive summary
    summary_df = create_comprehensive_summary()
    
    print(f"\n{'='*80}")
    print("TUTORIAL 08 COMPLETE!")
    print(f"{'='*80}")
    print(f"All results saved to: {flow_analysis_output_dir}")
    
    # List generated files
    print(f"\nGenerated files:")
    for file_path in sorted(flow_analysis_output_dir.glob('*')):
        print(f"  ðŸ“„ {file_path.name}")
    
    # Count files by type
    png_files = list(flow_analysis_output_dir.glob('*.png'))
    csv_files = list(flow_analysis_output_dir.glob('*.csv'))
    
    print(f"\nðŸ“Š Files summary:")
    print(f"   ðŸ“ˆ Plots: {len(png_files)} PNG files")
    print(f"   ðŸ“„ Data: {len(csv_files)} CSV files")
    print(f"   ðŸŽ¯ Total: {len(png_files) + len(csv_files)} files")
    
    print(f"\nðŸ’¡ Key Insights:")
    print(f"   â€¢ Flow injection (no-flow) provides cleaner, more predictable behavior")
    print(f"   â€¢ Flow substitution (baseline) shows complex interactions with learned patterns")
    print(f"   â€¢ Fractional flow distribution is architecturally superior for high flow speeds")
    print(f"   â€¢ Direct flow distribution becomes erratic at extreme flow values")

if __name__ == "__main__":
    main() 