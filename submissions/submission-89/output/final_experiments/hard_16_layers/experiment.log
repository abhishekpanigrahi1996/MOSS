2025-05-13 17:48:47 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-13 17:48:47 - __main__ - INFO - Setting up experiment
2025-05-13 17:48:47 - training.experiment - INFO - Created data loaders with batch sizes: train=64, val=64
2025-05-13 17:48:47 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-13 17:48:47 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-13 17:48:47 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-13 17:48:47 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-13 17:48:47 - model.transformer - INFO - Flow distribution mode: direct
2025-05-13 17:48:47 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-13 17:48:47 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-13 17:48:48 - training.trainer - INFO - TensorBoard logging enabled at output/final_experiments/hard_16_layers/tensorboard
2025-05-13 17:48:48 - training.experiment - INFO - Experiment setup completed
2025-05-13 17:48:48 - __main__ - INFO - Running experiment
2025-05-13 17:48:48 - training.trainer - INFO - Starting training for 80 epochs (from epoch 1 to 80)
2025-05-13 17:56:15 - training.trainer - INFO - Epoch 1/80 - Train Loss: 0.211171, Train Normalized Loss: 0.667781
2025-05-13 17:56:15 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 17:56:15 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 18:03:42 - training.trainer - INFO - Epoch 2/80 - Train Loss: 0.120097, Train Normalized Loss: 0.379780
2025-05-13 18:03:42 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 18:03:42 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 18:11:11 - training.trainer - INFO - Epoch 3/80 - Train Loss: 0.090516, Train Normalized Loss: 0.286237
2025-05-13 18:11:11 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 18:11:11 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 20:41:56 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-13 20:41:56 - __main__ - INFO - Setting up experiment
2025-05-13 20:41:56 - training.experiment - INFO - Created data loaders with batch sizes: train=64, val=64
2025-05-13 20:41:56 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-13 20:41:56 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-13 20:41:56 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-13 20:41:56 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-13 20:41:56 - model.transformer - INFO - Flow distribution mode: direct
2025-05-13 20:41:56 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-13 20:41:57 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-13 20:41:58 - training.trainer - INFO - TensorBoard logging enabled at output/final_experiments/hard_16_layers/tensorboard
2025-05-13 20:41:58 - training.experiment - INFO - Experiment setup completed
2025-05-13 20:41:58 - __main__ - INFO - Running experiment
2025-05-13 20:41:58 - training.trainer - INFO - Starting training for 80 epochs (from epoch 1 to 80)
2025-05-13 20:42:43 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-13 20:42:43 - __main__ - INFO - Setting up experiment
2025-05-13 20:42:43 - training.experiment - INFO - Created data loaders with batch sizes: train=64, val=64
2025-05-13 20:42:43 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-13 20:42:43 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-13 20:42:43 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-13 20:42:43 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-13 20:42:43 - model.transformer - INFO - Flow distribution mode: direct
2025-05-13 20:42:43 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-13 20:42:44 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-13 20:42:45 - training.trainer - INFO - TensorBoard logging enabled at output/final_experiments/hard_16_layers/tensorboard
2025-05-13 20:42:45 - training.experiment - INFO - Experiment setup completed
2025-05-13 20:42:45 - __main__ - INFO - Running experiment
2025-05-13 20:42:45 - training.trainer - INFO - Starting training for 80 epochs (from epoch 1 to 80)
2025-05-13 20:44:25 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-13 20:44:25 - __main__ - INFO - Setting up experiment
2025-05-13 20:44:25 - training.experiment - INFO - Created data loaders with batch sizes: train=64, val=64
2025-05-13 20:44:25 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-13 20:44:25 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-13 20:44:25 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-13 20:44:25 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-13 20:44:25 - model.transformer - INFO - Flow distribution mode: direct
2025-05-13 20:44:25 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-13 20:44:26 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-13 20:44:26 - training.trainer - INFO - TensorBoard logging enabled at output/final_experiments/hard_16_layers/tensorboard
2025-05-13 20:44:26 - training.experiment - INFO - Experiment setup completed
2025-05-13 20:44:26 - __main__ - INFO - Running experiment
2025-05-13 20:44:26 - training.trainer - INFO - Starting training for 80 epochs (from epoch 1 to 80)
2025-05-13 20:48:23 - training.trainer - INFO - Epoch 1/80 - Train Loss: 0.211171, Train Normalized Loss: 0.667781
2025-05-13 20:48:23 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 20:48:23 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 20:52:19 - training.trainer - INFO - Epoch 2/80 - Train Loss: 0.120097, Train Normalized Loss: 0.379780
2025-05-13 20:52:19 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 20:52:19 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 20:56:14 - training.trainer - INFO - Epoch 3/80 - Train Loss: 0.090518, Train Normalized Loss: 0.286243
2025-05-13 20:56:14 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 20:56:14 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:00:07 - training.trainer - INFO - Epoch 4/80 - Train Loss: 0.078765, Train Normalized Loss: 0.249077
2025-05-13 21:02:22 - training.trainer - INFO - Validation Loss: 0.070518, Validation Normalized Loss: 0.222997
2025-05-13 21:02:22 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:02:22 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:02:22 - training.trainer - INFO - Saved best model with validation loss: 0.070518
2025-05-13 21:02:22 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:02:22 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:06:22 - training.trainer - INFO - Epoch 5/80 - Train Loss: 0.068623, Train Normalized Loss: 0.217004
2025-05-13 21:06:22 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:06:22 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:10:16 - training.trainer - INFO - Epoch 6/80 - Train Loss: 0.061831, Train Normalized Loss: 0.195526
2025-05-13 21:10:16 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:10:16 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:14:12 - training.trainer - INFO - Epoch 7/80 - Train Loss: 0.058554, Train Normalized Loss: 0.185164
2025-05-13 21:14:12 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:14:12 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:18:03 - training.trainer - INFO - Epoch 8/80 - Train Loss: 0.055030, Train Normalized Loss: 0.174020
2025-05-13 21:20:15 - training.trainer - INFO - Validation Loss: 0.055331, Validation Normalized Loss: 0.174973
2025-05-13 21:20:15 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:20:15 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:20:15 - training.trainer - INFO - Saved best model with validation loss: 0.055331
2025-05-13 21:20:15 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:20:15 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:24:07 - training.trainer - INFO - Epoch 9/80 - Train Loss: 0.052970, Train Normalized Loss: 0.167505
2025-05-13 21:24:07 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:24:07 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:28:01 - training.trainer - INFO - Epoch 10/80 - Train Loss: 0.050735, Train Normalized Loss: 0.160439
2025-05-13 21:28:01 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:28:01 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:31:57 - training.trainer - INFO - Epoch 11/80 - Train Loss: 0.049477, Train Normalized Loss: 0.156459
2025-05-13 21:31:57 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:31:57 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:35:49 - training.trainer - INFO - Epoch 12/80 - Train Loss: 0.049148, Train Normalized Loss: 0.155418
2025-05-13 21:38:03 - training.trainer - INFO - Validation Loss: 0.057166, Validation Normalized Loss: 0.180774
2025-05-13 21:38:03 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:38:03 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:42:03 - training.trainer - INFO - Epoch 13/80 - Train Loss: 0.048403, Train Normalized Loss: 0.153063
2025-05-13 21:42:03 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:42:03 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:45:59 - training.trainer - INFO - Epoch 14/80 - Train Loss: 0.047445, Train Normalized Loss: 0.150035
2025-05-13 21:45:59 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:45:59 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 21:49:54 - training.trainer - INFO - Epoch 15/80 - Train Loss: 0.046075, Train Normalized Loss: 0.145703
2025-05-13 21:49:54 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 21:49:54 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:07:48 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-13 22:07:48 - __main__ - INFO - Setting device to cuda:1 for resumed experiment
2025-05-13 22:07:49 - training.experiment - INFO - Resumed training data loader from output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:07:49 - training.experiment - INFO - Resumed validation data loader from output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:07:49 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-13 22:07:49 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-13 22:07:49 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-13 22:07:49 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-13 22:07:49 - model.transformer - INFO - Flow distribution mode: direct
2025-05-13 22:07:49 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-13 22:07:49 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-13 22:07:50 - training.trainer - INFO - TensorBoard logging enabled at output/final_experiments/hard_16_layers/tensorboard
2025-05-13 22:07:50 - training.experiment - INFO - Experiment setup completed
2025-05-13 22:07:50 - training.trainer - INFO - Loaded checkpoint from output/final_experiments/hard_16_layers/checkpoints/latest_model.pt (epoch 15)
2025-05-13 22:07:50 - training.experiment - INFO - Will continue training for remaining 65 epochs
2025-05-13 22:07:50 - training.trainer - INFO - Loaded checkpoint from output/final_experiments/hard_16_layers/checkpoints/latest_model.pt (epoch 15)
2025-05-13 22:07:50 - training.trainer - INFO - Starting training for 65 epochs (from epoch 16 to 80)
2025-05-13 22:11:45 - training.trainer - INFO - Epoch 16/80 - Train Loss: 0.046570, Train Normalized Loss: 0.147268
2025-05-13 22:13:59 - training.trainer - INFO - Validation Loss: 0.057970, Validation Normalized Loss: 0.183316
2025-05-13 22:13:59 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:13:59 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:17:57 - training.trainer - INFO - Epoch 17/80 - Train Loss: 0.046635, Train Normalized Loss: 0.147473
2025-05-13 22:17:57 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:17:57 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:21:52 - training.trainer - INFO - Epoch 18/80 - Train Loss: 0.045063, Train Normalized Loss: 0.142501
2025-05-13 22:21:52 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:21:52 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:25:47 - training.trainer - INFO - Epoch 19/80 - Train Loss: 0.045552, Train Normalized Loss: 0.144048
2025-05-13 22:25:47 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:25:47 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:29:44 - training.trainer - INFO - Epoch 20/80 - Train Loss: 0.043767, Train Normalized Loss: 0.138404
2025-05-13 22:31:59 - training.trainer - INFO - Validation Loss: 0.046573, Validation Normalized Loss: 0.147276
2025-05-13 22:31:59 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:31:59 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:31:59 - training.trainer - INFO - Saved best model with validation loss: 0.046573
2025-05-13 22:31:59 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:31:59 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:35:52 - training.trainer - INFO - Epoch 21/80 - Train Loss: 0.043572, Train Normalized Loss: 0.137787
2025-05-13 22:35:52 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:35:52 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:39:46 - training.trainer - INFO - Epoch 22/80 - Train Loss: 0.044386, Train Normalized Loss: 0.140362
2025-05-13 22:39:46 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:39:46 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:43:45 - training.trainer - INFO - Epoch 23/80 - Train Loss: 0.044453, Train Normalized Loss: 0.140571
2025-05-13 22:43:45 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:43:45 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:47:39 - training.trainer - INFO - Epoch 24/80 - Train Loss: 0.043549, Train Normalized Loss: 0.137713
2025-05-13 22:49:49 - training.trainer - INFO - Validation Loss: 0.047721, Validation Normalized Loss: 0.150907
2025-05-13 22:49:49 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:49:49 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:53:36 - training.trainer - INFO - Epoch 25/80 - Train Loss: 0.043656, Train Normalized Loss: 0.138051
2025-05-13 22:53:36 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:53:36 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 22:57:32 - training.trainer - INFO - Epoch 26/80 - Train Loss: 0.043584, Train Normalized Loss: 0.137826
2025-05-13 22:57:32 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 22:57:32 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:01:29 - training.trainer - INFO - Epoch 27/80 - Train Loss: 0.043175, Train Normalized Loss: 0.136531
2025-05-13 23:01:29 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:01:29 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:05:29 - training.trainer - INFO - Epoch 28/80 - Train Loss: 0.043572, Train Normalized Loss: 0.137786
2025-05-13 23:07:40 - training.trainer - INFO - Validation Loss: 0.045030, Validation Normalized Loss: 0.142398
2025-05-13 23:07:40 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:07:40 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:07:40 - training.trainer - INFO - Saved best model with validation loss: 0.045030
2025-05-13 23:07:40 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:07:40 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:11:36 - training.trainer - INFO - Epoch 29/80 - Train Loss: 0.043328, Train Normalized Loss: 0.137014
2025-05-13 23:11:36 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:11:36 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:15:30 - training.trainer - INFO - Epoch 30/80 - Train Loss: 0.041135, Train Normalized Loss: 0.130081
2025-05-13 23:15:30 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:15:30 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:19:23 - training.trainer - INFO - Epoch 31/80 - Train Loss: 0.042428, Train Normalized Loss: 0.134168
2025-05-13 23:19:23 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:19:23 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:23:16 - training.trainer - INFO - Epoch 32/80 - Train Loss: 0.041708, Train Normalized Loss: 0.131893
2025-05-13 23:25:28 - training.trainer - INFO - Validation Loss: 0.053163, Validation Normalized Loss: 0.168117
2025-05-13 23:25:28 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:25:28 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:29:23 - training.trainer - INFO - Epoch 33/80 - Train Loss: 0.042473, Train Normalized Loss: 0.134311
2025-05-13 23:29:23 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:29:23 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:33:18 - training.trainer - INFO - Epoch 34/80 - Train Loss: 0.043320, Train Normalized Loss: 0.136989
2025-05-13 23:33:18 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:33:18 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:37:09 - training.trainer - INFO - Epoch 35/80 - Train Loss: 0.041584, Train Normalized Loss: 0.131499
2025-05-13 23:37:09 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:37:09 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:41:07 - training.trainer - INFO - Epoch 36/80 - Train Loss: 0.041203, Train Normalized Loss: 0.130297
2025-05-13 23:43:20 - training.trainer - INFO - Validation Loss: 0.048424, Validation Normalized Loss: 0.153130
2025-05-13 23:43:20 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:43:20 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:47:15 - training.trainer - INFO - Epoch 37/80 - Train Loss: 0.040747, Train Normalized Loss: 0.128855
2025-05-13 23:47:15 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:47:15 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:51:07 - training.trainer - INFO - Epoch 38/80 - Train Loss: 0.040436, Train Normalized Loss: 0.127870
2025-05-13 23:51:07 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:51:07 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:55:01 - training.trainer - INFO - Epoch 39/80 - Train Loss: 0.040378, Train Normalized Loss: 0.127687
2025-05-13 23:55:01 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-13 23:55:01 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-13 23:58:50 - training.trainer - INFO - Epoch 40/80 - Train Loss: 0.039308, Train Normalized Loss: 0.124303
2025-05-14 00:01:04 - training.trainer - INFO - Validation Loss: 0.049001, Validation Normalized Loss: 0.154954
2025-05-14 00:01:04 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:01:04 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:05:00 - training.trainer - INFO - Epoch 41/80 - Train Loss: 0.040055, Train Normalized Loss: 0.126665
2025-05-14 00:05:00 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:05:00 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:08:51 - training.trainer - INFO - Epoch 42/80 - Train Loss: 0.040102, Train Normalized Loss: 0.126813
2025-05-14 00:08:51 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:08:51 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:12:45 - training.trainer - INFO - Epoch 43/80 - Train Loss: 0.039628, Train Normalized Loss: 0.125316
2025-05-14 00:12:45 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:12:45 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:16:40 - training.trainer - INFO - Epoch 44/80 - Train Loss: 0.038345, Train Normalized Loss: 0.121258
2025-05-14 00:18:52 - training.trainer - INFO - Validation Loss: 0.041806, Validation Normalized Loss: 0.132202
2025-05-14 00:18:52 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:18:52 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:18:52 - training.trainer - INFO - Saved best model with validation loss: 0.041806
2025-05-14 00:18:52 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:18:52 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:22:48 - training.trainer - INFO - Epoch 45/80 - Train Loss: 0.039019, Train Normalized Loss: 0.123389
2025-05-14 00:22:48 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:22:48 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:26:42 - training.trainer - INFO - Epoch 46/80 - Train Loss: 0.039451, Train Normalized Loss: 0.124755
2025-05-14 00:26:42 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:26:42 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:30:37 - training.trainer - INFO - Epoch 47/80 - Train Loss: 0.039512, Train Normalized Loss: 0.124949
2025-05-14 00:30:37 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:30:37 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:34:28 - training.trainer - INFO - Epoch 48/80 - Train Loss: 0.037838, Train Normalized Loss: 0.119655
2025-05-14 00:36:40 - training.trainer - INFO - Validation Loss: 0.048101, Validation Normalized Loss: 0.152108
2025-05-14 00:36:40 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:36:40 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:40:34 - training.trainer - INFO - Epoch 49/80 - Train Loss: 0.039409, Train Normalized Loss: 0.124621
2025-05-14 00:40:34 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:40:34 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:44:30 - training.trainer - INFO - Epoch 50/80 - Train Loss: 0.037633, Train Normalized Loss: 0.119005
2025-05-14 00:44:30 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:44:30 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:48:25 - training.trainer - INFO - Epoch 51/80 - Train Loss: 0.038380, Train Normalized Loss: 0.121369
2025-05-14 00:48:25 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:48:25 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:52:17 - training.trainer - INFO - Epoch 52/80 - Train Loss: 0.038824, Train Normalized Loss: 0.122773
2025-05-14 00:54:28 - training.trainer - INFO - Validation Loss: 0.045404, Validation Normalized Loss: 0.143579
2025-05-14 00:54:28 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:54:28 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 00:58:21 - training.trainer - INFO - Epoch 53/80 - Train Loss: 0.037735, Train Normalized Loss: 0.119330
2025-05-14 00:58:21 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 00:58:21 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:02:15 - training.trainer - INFO - Epoch 54/80 - Train Loss: 0.036862, Train Normalized Loss: 0.116567
2025-05-14 01:02:15 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:02:15 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:06:06 - training.trainer - INFO - Epoch 55/80 - Train Loss: 0.037823, Train Normalized Loss: 0.119607
2025-05-14 01:06:06 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:06:06 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:10:00 - training.trainer - INFO - Epoch 56/80 - Train Loss: 0.037403, Train Normalized Loss: 0.118279
2025-05-14 01:12:14 - training.trainer - INFO - Validation Loss: 0.045167, Validation Normalized Loss: 0.142829
2025-05-14 01:12:14 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:12:14 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:16:10 - training.trainer - INFO - Epoch 57/80 - Train Loss: 0.037705, Train Normalized Loss: 0.119233
2025-05-14 01:16:10 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:16:10 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:20:03 - training.trainer - INFO - Epoch 58/80 - Train Loss: 0.036827, Train Normalized Loss: 0.116457
2025-05-14 01:20:03 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:20:03 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:23:59 - training.trainer - INFO - Epoch 59/80 - Train Loss: 0.036529, Train Normalized Loss: 0.115515
2025-05-14 01:23:59 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:23:59 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:27:55 - training.trainer - INFO - Epoch 60/80 - Train Loss: 0.037371, Train Normalized Loss: 0.118178
2025-05-14 01:30:08 - training.trainer - INFO - Validation Loss: 0.043800, Validation Normalized Loss: 0.138508
2025-05-14 01:30:08 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:30:08 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:34:02 - training.trainer - INFO - Epoch 61/80 - Train Loss: 0.036934, Train Normalized Loss: 0.116796
2025-05-14 01:34:02 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:34:02 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:38:00 - training.trainer - INFO - Epoch 62/80 - Train Loss: 0.037971, Train Normalized Loss: 0.120075
2025-05-14 01:38:00 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:38:00 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:41:56 - training.trainer - INFO - Epoch 63/80 - Train Loss: 0.037123, Train Normalized Loss: 0.117393
2025-05-14 01:41:56 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:41:56 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:45:49 - training.trainer - INFO - Epoch 64/80 - Train Loss: 0.038149, Train Normalized Loss: 0.120638
2025-05-14 01:48:00 - training.trainer - INFO - Validation Loss: 0.045789, Validation Normalized Loss: 0.144798
2025-05-14 01:48:00 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:48:00 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:51:54 - training.trainer - INFO - Epoch 65/80 - Train Loss: 0.038126, Train Normalized Loss: 0.120565
2025-05-14 01:51:54 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:51:54 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:55:46 - training.trainer - INFO - Epoch 66/80 - Train Loss: 0.036315, Train Normalized Loss: 0.114838
2025-05-14 01:55:46 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:55:46 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 01:59:43 - training.trainer - INFO - Epoch 67/80 - Train Loss: 0.036984, Train Normalized Loss: 0.116953
2025-05-14 01:59:43 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 01:59:43 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:03:36 - training.trainer - INFO - Epoch 68/80 - Train Loss: 0.036816, Train Normalized Loss: 0.116423
2025-05-14 02:05:48 - training.trainer - INFO - Validation Loss: 0.042346, Validation Normalized Loss: 0.133909
2025-05-14 02:05:48 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:05:48 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:09:45 - training.trainer - INFO - Epoch 69/80 - Train Loss: 0.036479, Train Normalized Loss: 0.115355
2025-05-14 02:09:45 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:09:45 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:13:36 - training.trainer - INFO - Epoch 70/80 - Train Loss: 0.036534, Train Normalized Loss: 0.115529
2025-05-14 02:13:36 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:13:36 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:17:31 - training.trainer - INFO - Epoch 71/80 - Train Loss: 0.035103, Train Normalized Loss: 0.111006
2025-05-14 02:17:31 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:17:31 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:21:26 - training.trainer - INFO - Epoch 72/80 - Train Loss: 0.037594, Train Normalized Loss: 0.118883
2025-05-14 02:23:37 - training.trainer - INFO - Validation Loss: 0.047694, Validation Normalized Loss: 0.150823
2025-05-14 02:23:37 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:23:37 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:27:30 - training.trainer - INFO - Epoch 73/80 - Train Loss: 0.036787, Train Normalized Loss: 0.116330
2025-05-14 02:27:30 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:27:30 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:31:28 - training.trainer - INFO - Epoch 74/80 - Train Loss: 0.036046, Train Normalized Loss: 0.113989
2025-05-14 02:31:28 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:31:28 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:35:23 - training.trainer - INFO - Epoch 75/80 - Train Loss: 0.036418, Train Normalized Loss: 0.115164
2025-05-14 02:35:23 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:35:23 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:39:17 - training.trainer - INFO - Epoch 76/80 - Train Loss: 0.036426, Train Normalized Loss: 0.115190
2025-05-14 02:41:31 - training.trainer - INFO - Validation Loss: 0.042937, Validation Normalized Loss: 0.135777
2025-05-14 02:41:31 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:41:31 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:45:26 - training.trainer - INFO - Epoch 77/80 - Train Loss: 0.036732, Train Normalized Loss: 0.116156
2025-05-14 02:45:26 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:45:26 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:49:22 - training.trainer - INFO - Epoch 78/80 - Train Loss: 0.036710, Train Normalized Loss: 0.116088
2025-05-14 02:49:22 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:49:22 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:53:19 - training.trainer - INFO - Epoch 79/80 - Train Loss: 0.036745, Train Normalized Loss: 0.116198
2025-05-14 02:53:19 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:53:19 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:57:11 - training.trainer - INFO - Epoch 80/80 - Train Loss: 0.037076, Train Normalized Loss: 0.117246
2025-05-14 02:59:27 - training.trainer - INFO - Validation Loss: 0.043744, Validation Normalized Loss: 0.138331
2025-05-14 02:59:27 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:59:27 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:59:27 - training.trainer - INFO - Saved training data loader state to output/final_experiments/hard_16_layers/data_state.json
2025-05-14 02:59:27 - training.trainer - INFO - Saved validation data loader state to output/final_experiments/hard_16_layers/val_data_state.json
2025-05-14 02:59:27 - training.trainer - INFO - Saved final checkpoint
2025-05-14 02:59:27 - training.experiment - INFO - Training completed in 17496.37 seconds
2025-05-14 02:59:27 - __main__ - INFO - Resumed experiment hard_16_layers completed successfully
2025-05-14 15:39:24 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-14 15:39:24 - training.experiment - INFO - Created data loaders with batch sizes: train=64, val=64
2025-05-14 15:39:24 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-14 15:39:24 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-14 15:39:24 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-14 15:39:24 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-14 15:39:24 - model.transformer - INFO - Flow distribution mode: direct
2025-05-14 15:39:24 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-14 15:39:24 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-15 02:34:02 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-15 02:34:02 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-15 02:34:02 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-15 02:34:02 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-15 02:34:02 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-15 02:34:02 - model.transformer - INFO - Flow distribution mode: direct
2025-05-15 02:34:02 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-15 02:34:02 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-15 02:34:53 - training.experiment - INFO - Experiment 'hard_16_layers' initialized with ID: hard_16_layers
2025-05-15 02:34:53 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-15 02:34:53 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-15 02:34:53 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-15 02:34:53 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-15 02:34:53 - model.transformer - INFO - Flow distribution mode: direct
2025-05-15 02:34:53 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-15 02:34:53 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
