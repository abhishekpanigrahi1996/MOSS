# Encoderâ€“Decoder & Flowâ€‘Speed System

> Detailed reference for the **speedâ€‘conditioned encoderâ€“decoder** stack (`SpeedEncDec`, `SpeedLayerNormPow`) and the **flowâ€‘speed prediction** modules used by the GMM Transformer framework.

---

## 1Â Â Motivation

Traditional transformer pipelines first embed **discrete tokens** into a latent space. In pointâ€‘cloud/GMM settings, inputs are **continuous coordinates**. We therefore:

* Replace fixed embeddings with an **orthogonal encoderâ€“decoder pair** that can learn rotations and (optionally) learnable scaling while preserving invertibility.
* Introduce a **flowâ€‘speed \( s\in[0,1] \)** that adaptively modulates the *magnitude* of residual updates. Higherâ€‘quality (high SNR) inputs require minor updates, lowerâ€‘quality samples require stronger corrections.

---

## 2Â Â Speedâ€‘Conditioned Encoderâ€“Decoder

### 2.1Â Â Architecture

```
        X âˆˆ â„^{BÃ—NÃ—d}                 (raw coordinates)
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   U,V = orthogonal matrices
    â”‚   SpeedEncDec.encode â”‚   Bâ‚€  = nullâ€‘space basis
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Ïƒ    = learnable logâ€‘scale
               â”‚ z âˆˆ â„^{BÃ—NÃ—D}
               â–¼
        Transformer â€¦        (contextual processing)
               â”‚ h
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SpeedEncDec.decode â”‚  (uses same parameters)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Å¶  (normalised prediction)
```

`SpeedEncDec(d, D)` holds parameters:

| Symbol | Shape | Role |
|--------|-------|------|
| \(U\) | \(DÃ—d\) | projects input into latent space |
| \(V\) | \(dÃ—d\) | rotation in original space |
| \(B_0\) | \(dÃ—D\) | projects latent nullâ€‘space back to input dim |
| \(\log Ïƒ âˆˆ â„^{d}\) | (optional) perâ€‘dimension scale parameters |
| \(A âˆˆ ğ”°ğ”¬(D)\) | skewâ€‘symmetric matrix for additional rotation (currently inactive) |

All linear factors are **orthogonal** by parametrisation with Torch's Cayley map â†’ preserves norm stability.

### 2.2Â Â Mathematics

Encoding (current implementation):
\[
 z = X U^âŠ¤ \tag{1}
\]

Decoding:
1. Project back to data space: \( P = z U \)
2. Rotate via \( V \): \( Y = P V^âŠ¤ \)
3. Recover nullâ€‘space component
\[
 z_{âŠ¥} = z - P U^âŠ¤,\qquad N = z_{âŠ¥} B_0^âŠ¤
\]
4. Output:
\[
 \widehat X = Y + N \tag{2}
\]

**Identity when \(s=0\)** â€“ scaling & rotation branches are multiplied by speedâ€‘dependent factors that vanish at zero (see commented code for potential extensions).

### 2.3Â Â Properties

* Invertible under full parameterisation when scale term is active.
* Lipschitzâ€‘stable due to orthogonality.
* Guarantees \(\widehat X=X\) at speed 0 â†’ curriculum learning friendly.

---

## 3Â Â Speedâ€‘Conditioned Layer Normalisation

`SpeedLayerNormPow` provides a *soft transition* between identity and standard LayerNorm:

\[
\text{SLN}(x,t) = Î³(t)\;x + Î²(t),\qquad t\in[0,1]
\]
where
\[
 Î³(t) = \exp(t\;Î³),\qquad Î²(t) = t\;Î².
\]
Here \(Î³,Î²\) are learnable vectors. At \(t=0\) the operation is identity; at \(t=1\) it equals an affine reâ€‘scaling. (The usual centring/variance terms are currently skipped to keep variance information intact.)

---

## 4Â Â Flowâ€‘Speed Prediction

Flowâ€‘speed \(s\) modulates residual connections inside each `TransformerBlock`:
\[
 Y = X + s\;Î”(X),\qquad 0â‰¤sâ‰¤1. \tag{3}
\]
### 4.1Â Â Interfaces

```python
flow = predictor(targets, inputs)  # returns Tensor[B] or Tensor[B,L]
```
* **Global flow** â€“ shape \([B]\)
* **Perâ€‘layer flow** â€“ shape \([B,L]\)

### 4.2Â Â Predictor Implementations

| Class | Idea | Equation |
|-------|------|----------|
| `DummyFlowPredictor` | Always 1 | \(s=1\) |
| `LinearFlowPredictor` | Affine map on SNR (clipped) | \(
 s = s_{min} + (s_{max}-s_{min})\;\frac{\text{SNR}_{max}-\text{SNR}}{\text{SNR}_{max}-\text{SNR}_{min}}\)
| `MonotonicFlowPredictor` | Learn monotone spline (pieceâ€‘wise linear) | see Eq. (4) |

Pieceâ€‘wise spline (per layer or shared):
\[
 s(x)=h_i+\frac{h_{i+1}-h_i}{t_{i+1}-t_i}(x-t_i),\qquad xâˆˆ[t_i,t_{i+1}) \tag{4}
\]
with learned nonâ€‘negative height deltas \(Î”h_i\) guaranteeing monotonicity and normalised so \(s\in[0,1]\).

### 4.3Â Â Fractional Flow Distribution

When layers are **virtually repeated** (cycle/layerwise/grouped), flow can be *fractionally* distributed:

\[
 s_j = \begin{cases}
 1,& j < âŒŠk sâŒ‹\\
 k s - âŒŠk sâŒ‹,& j = âŒŠk sâŒ‹\\
 0,& \text{otherwise}
 \end{cases} \tag{5}
\]
where \(k\) = repetition factor, \(j\) = current replica index. This enables subâ€‘unit application counts (e.g., effective 0.3 passes).

---

## 5Â Â Interaction Summary

1. **Encoder/Norm/Decoder** read **local** flow (scalar per sample) â€“ typically the average of perâ€‘layer flow.
2. **Transformer blocks** consume (possibly perâ€‘layer) flow via Eq. (3).
3. Predictors map **observable difficulty** (SNR) â†’ flow in a *monotone* fashion, ensuring that better data gets gentler updates.

---

## 6Â Â Key Advantages

* **Curriculumâ€‘friendly** â€“ identity path at \(s=0\) stabilises early training.
* **Efficiency** â€“ dynamic residual strength reduces wasted computation on easy examples.
* **Extensibility** â€“ dropâ€‘in new predictor with minimal code changes. 